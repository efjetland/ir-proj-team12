{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "import elasticsearch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pytest\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import json\n",
    "import bz2file\n",
    "import bz2\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from bz2 import BZ2File as bzopen\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Query from Github\n",
    "\n",
    "Download query from github and split into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14056\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/datasets/DBpedia/'\n",
    "file = 'smarttask_dbpedia_train.json'\n",
    "url = url + file\n",
    "queries=requests.get(url).json()\n",
    "\n",
    "\n",
    "random.seed(a=1234567)\n",
    "TRAIN_SIZE = int(len(queries) * 0.8)\n",
    "\n",
    "TRAIN_QUERY = queries[:TRAIN_SIZE]\n",
    "TEST_QUERY = queries[TRAIN_SIZE:]\n",
    "\n",
    "print(len(TRAIN_QUERY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file from DBPedia NLP dataset\n",
    "\n",
    "Check whether dataset files (bz2) are available in data folder and download bz2 file if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburl= 'http://downloads.dbpedia.org/2016-10/core-i18n/en/'\n",
    "#dbfiles = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2','instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "#cwd = os.getcwd()  \n",
    "dbfiles = ['long_abstracts_en.ttl.bz2','instance_types_transitive_en.ttl.bz2']\n",
    "dirname = 'data'\n",
    "\n",
    "for dbfile in dbfiles:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname) \n",
    "        \n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "        # Download file\n",
    "        url = dburl + dbfile\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Index configuration \n",
    "\n",
    "For each of the fields, store the term vectors. These should be stored in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "INDEX_NAME = 'nlp_entity'\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                #'names': {\n",
    "                #    'type': 'text',\n",
    "                #    'term_vector': 'yes',\n",
    "                #    'analyzer': 'english'\n",
    "                #},\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'types': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                #'catch_all': {\n",
    "                #    'type': 'text',\n",
    "                #    'term_vector': 'yes',\n",
    "                #    'analyzer': 'english'\n",
    "                #},\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS = '<http://dbpedia.org/ontology/abstract>'\n",
    "COMMENTS = '<http://www.w3.org/2000/01/rdf-schema#comment>'\n",
    "TYPES = '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
    "#abstract_files = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2']\n",
    "#type_files = ['instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "abstract_files = ['long_abstracts_en.ttl.bz2']\n",
    "type_files = ['instance_types_transitive_en.ttl.bz2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abstract(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    text = text[1].replace('\"', ' ')\n",
    "    text = text.replace(' @en .','')\n",
    "    return text\n",
    "\n",
    "def resolve_entity(text):\n",
    "    name = text.split('>')[0].replace('_', ' ')\n",
    "    name = name.split('/')[-1]\n",
    "    return name\n",
    "\n",
    "def resolve_types(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    types = text[1]    \n",
    "    types = types[types.rfind('/')+1:types.rfind('>')]\n",
    "    return 'dbo:'+types\n",
    "\n",
    "def populate_type(idx, filename):\n",
    "    doc = {}\n",
    "    #cwd = os.getcwd()\n",
    "    #myfile= os.path.join(cwd, dbfile)\n",
    "    myfile = 'data' + '/' +  filename\n",
    "    with bzopen(myfile, \"r\") as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 0: continue                #skip header\n",
    "            line = line.rstrip().decode('utf-8')\n",
    "            entity = resolve_entity(line)\n",
    "\n",
    "            if entity is not None:\n",
    "                if TYPES in line:                         #update index if it already exist if not then create new one    \n",
    "                    types=resolve_types(line,TYPES)\n",
    "                    if es.exists(index=idx, id=entity):\n",
    "                        result = es.update(index=idx, id=entity, body={\n",
    "                            'script':{'source':'ctx._source.types.add(params.new_type)', 'params':\n",
    "                                          {'new_type' : types}}})['result']\n",
    "                        #result = es.update(index=idx, id=entity, body={\n",
    "                        #    'script':{'source':'if(ctx._source.types.contains(params.new_type)){ctx._source.types=ctx._source.types} else {ctx._source.types.add(params.new_type)}', \n",
    "                        #    'params': {'new_type' : types}}})['result']\n",
    "\n",
    "                    else:\n",
    "                        es.index(index=idx, id=entity, body={'types':[types],'description':''})\n",
    "                      \n",
    "            if i == 100: break\n",
    "\n",
    "def populate_abstract(idx, filename):\n",
    "    doc = {}\n",
    "    #cwd = os.getcwd()\n",
    "    #myfile= os.path.join(cwd, filename)\n",
    "    myfile = 'data' + '/'  + filename\n",
    "    with bzopen(myfile, \"r\") as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 0: continue                      #skip header\n",
    "            line = line.rstrip().decode('utf-8')\n",
    "            entity = resolve_entity(line)\n",
    "            \n",
    "            if entity is not None:\n",
    "                if ABSTRACTS in line:     \n",
    "                    desc = resolve_abstract(line,ABSTRACTS)\n",
    "                elif COMMENTS in line:\n",
    "                    desc = resolve_abstract(line,COMMENTS)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if es.exists(index=idx, id=entity):    #only update index if already exist \n",
    "                    result = es.update(index=idx, id=entity, body={\n",
    "                            'script':{'source':'ctx._source.description += params.new_desc', \n",
    "                                      'params': {'new_desc' :desc + ' '}}})['result']\n",
    "\n",
    "                    #result =es.update(index=idx, id=entity, body={\n",
    "                    #        'script':{'source':'if(ctx._source.description.contains(params.new_desc)){ctx._source.description=ctx._source.description} else {ctx._source.description += params.new_desc}', \n",
    "                    #        'params': {'new_desc' : ' ' + desc}}})['result']\n",
    "                    if result != 'updated': print('failed to update description for :', entity)\n",
    "                    \n",
    "               # else:\n",
    "               #     print(entity)\n",
    "               #     es.index(index=idx, id=entity, body={'types':[],'description':desc})\n",
    "\n",
    "            if i == 100: break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_entity'}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate index\n",
    "\n",
    "There are 2 steps in populating index based on fields. This is due to fact that there are 3 different dataset files for 2 different fields, types and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in type_files:\n",
    "    populate_type(INDEX_NAME,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in abstract_files:\n",
    "    populate_abstract(INDEX_NAME, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'took': 0, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 4.552845, 'hits': [{'_index': 'nlp_entity', '_type': '_doc', '_id': 'Actrius', '_score': 4.552845, '_source': {'types': ['dbo:Movie', 'dbo:Wikidata:Q11424', 'dbo:Work', 'dbo:CreativeWork', 'dbo:Q386724', 'dbo:owl#Thing', 'dbo:Movie', 'dbo:Wikidata:Q11424', 'dbo:Work', 'dbo:CreativeWork', 'dbo:Q386724', 'dbo:owl#Thing'], 'description': '  Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996.   Actresses (Catalan: Actrius) is a 1997 Catalan language Spanish drama film produced and directed by Ventura Pons and based on the award-winning stage play E.R. by Josep Maria Benet i Jornet. The film has no male actors, with all roles played by females. The film was produced in 1996. '}}]}}\n{'_index': 'nlp_entity', '_type': '_doc', '_id': 'Actrius', '_version': 14, 'found': True, 'took': 0, 'term_vectors': {'types': {'field_statistics': {'sum_doc_freq': 1553, 'doc_count': 226, 'sum_ttf': 2555}, 'terms': {'dbo:creativework': {'term_freq': 2}, 'dbo:movi': {'term_freq': 2}, 'dbo:owl': {'term_freq': 2}, 'dbo:q386724': {'term_freq': 2}, 'dbo:wikidata:q11424': {'term_freq': 2}, 'dbo:work': {'term_freq': 2}, 'thing': {'term_freq': 2}}}}}\n"
     ]
    }
   ],
   "source": [
    "query = 'Actrius'\n",
    "hits = es.search(index=INDEX_NAME, body={'query': {'match': {'description': query}}},\n",
    "                               _source=True, size=1)\n",
    "print(hits)\n",
    "tv_1 = es.termvectors(index=INDEX_NAME, id='Actrius', fields='types')\n",
    "print(tv_1)"
   ]
  },
  {
   "source": [
    "## Predic types for queries\n",
    "\n",
    "First score documents given the querie. Count the number of times a type occurs across the documents, dividing it by the sum of all type counts to get the type weight. For every type in a document, multiply the type weight with the document score and add to the types score."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_retrieval(index_name, query, field='description', k=100):\n",
    "    es_query = {\n",
    "        \"query\":{\n",
    "            \"query_string\":{\n",
    "                \"query\": query,\n",
    "                \"default_field\": field\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matches = es.search(index=index_name, body=es_query, _source=True, size=k)['hits']['hits']\n",
    "\n",
    "    type_count = defaultdict(int)\n",
    "    for match in matches:\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_count[doc_type] += 1\n",
    "\n",
    "    type_weight = {}\n",
    "    tc_sum = sum(type_count.values())\n",
    "    for t, c in type_count.items():\n",
    "        type_weight[t] = c/tc_sum\n",
    "\n",
    "    type_score = defaultdict(int)\n",
    "    for match in matches:\n",
    "        doc_id = match['_id']\n",
    "        doc_score = match['_score']\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_score[doc_type] += doc_score * type_weight[doc_type]\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_score.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-75466f29e6e8>, line 12)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-75466f29e6e8>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    query_scores[query['id']['actual']] = query['type']\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "query_scores = {}\n",
    "pattern = re.compile(r'[\\W_]+')\n",
    "for query in TRAIN_QUERY:\n",
    "    if query['category'] != 'resource':\n",
    "        continue\n",
    "    if query['question'] == None:\n",
    "        continue\n",
    "    q = pattern.sub(' ', query['question'])\n",
    "    q_scores = baseline_retrieval(INDEX_NAME, q)\n",
    "    predicted = q_scores[0:5]\n",
    "    query_scores[query['id']] = {'predicted': q_scores, 'actual': query['type']}\n",
    "query_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}