{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "import elasticsearch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pytest\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import json\n",
    "import bz2file\n",
    "import bz2\n",
    "import re\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from bz2 import BZ2File as bzopen\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Query from Github\n",
    "\n",
    "Download query from github and split into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14056\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/datasets/DBpedia/'\n",
    "file = 'smarttask_dbpedia_train.json'\n",
    "url = url + file\n",
    "queries=requests.get(url).json()\n",
    "\n",
    "\n",
    "random.seed(a=1234567)\n",
    "TRAIN_SIZE = int(len(queries) * 0.8)\n",
    "\n",
    "TRAIN_QUERY = queries[:TRAIN_SIZE]\n",
    "TEST_QUERY = queries[TRAIN_SIZE:]\n",
    "\n",
    "print(len(TRAIN_QUERY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file from DBPedia NLP dataset\n",
    "\n",
    "Check whether dataset files (bz2) are available in data folder and download bz2 file if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburl= 'http://downloads.dbpedia.org/2016-10/core-i18n/en/'\n",
    "#dbfiles = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2','instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "#cwd = os.getcwd()  \n",
    "dbfiles = ['long_abstracts_en.ttl.bz2','instance_types_transitive_en.ttl.bz2']\n",
    "dirname = 'data'\n",
    "\n",
    "for dbfile in dbfiles:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname) \n",
    "    \n",
    "    # Download file\n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "        url = dburl + dbfile\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    \n",
    "    #extract\n",
    "    if not os.path.exists(dirname + '/' + dbfile[:-4]):\n",
    "        filepath= dirname + '/'+ dbfile\n",
    "        newfilepath = filepath[:-4] \n",
    "        with open(filepath, 'rb') as source, open(newfilepath, 'wb') as dest:\n",
    "            dest.write(bz2.decompress(source.read()))\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Index configuration \n",
    "\n",
    "For each of the fields, store the term vectors. These should be stored in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "INDEX_NAME_ENTITY = 'nlp_entity'\n",
    "\n",
    "INDEX_SETTINGS_ENTITY = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'types': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "INDEX_NAME_TYPE = 'nlp_type'\n",
    "\n",
    "INDEX_SETTINGS_TYPE = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS = '<http://dbpedia.org/ontology/abstract>'\n",
    "COMMENTS = '<http://www.w3.org/2000/01/rdf-schema#comment>'\n",
    "TYPES = '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
    "#abstract_files = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2']\n",
    "#type_files = ['instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "abstract_files = ['long_abstracts_en.ttl']\n",
    "type_files = ['instance_types_transitive_en.ttl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abstract(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    text = text[1].replace('\"', '')\n",
    "    text = text.replace('@en .','')\n",
    "    return text\n",
    "\n",
    "def resolve_entity(text):\n",
    "    name = text.split('>')[0].replace('_', ' ')\n",
    "    name = name.split('/')[-1]\n",
    "    return name\n",
    "\n",
    "def resolve_types(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    types = text[1]    \n",
    "    types = types[types.rfind('/')+1:types.rfind('>')]\n",
    "    if 'wikidata' in types.lower(): return None\n",
    "    return 'dbo:'+types\n",
    "\n",
    "def populate_type(idx, filename):\n",
    "    doc = {}\n",
    "    with open(filename,'r',encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            #skip header\n",
    "            if i == 0: continue        \n",
    "            #only consider dbpedia\n",
    "            if 'dbpedia.org' in line.lower(): \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line)\n",
    "                if entity is not None:\n",
    "                    if TYPES in line:                                \n",
    "                        types=resolve_types(line,TYPES) \n",
    "                        #update index if it already exist if not then create new one \n",
    "                        try:\n",
    "                            if es.exists(index=idx, id=entity):\n",
    "                                result = es.update(index=idx, id=entity, body={\n",
    "                                    'script':{'source':'ctx._source.types.add(params.new_type)', 'params':\n",
    "                                                  {'new_type' : types}}})['result']\n",
    "                            else:\n",
    "                                es.index(index=idx, id=entity, body={'types':[types],'description':''})\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "            if i == 100: break\n",
    "\n",
    "def populate_abstract(idx, filename):\n",
    "    doc = {}\n",
    "    with open(filename, 'r', encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            #skip header\n",
    "            if i == 0: continue      \n",
    "            #only consider dbpedia\n",
    "            if 'dbpedia.org' in line.lower():            \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line) \n",
    "                if entity is not None:\n",
    "                    if ABSTRACTS in line:     \n",
    "                        desc = resolve_abstract(line,ABSTRACTS)\n",
    "                    elif COMMENTS in line:\n",
    "                        desc = resolve_abstract(line,COMMENTS)\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    #only update index if document type already exist \n",
    "                    try:\n",
    "                        if es.exists(index=idx, id=entity):                 \n",
    "                            result = es.update(index=idx, id=entity, body={\n",
    "                                    'script':{'source':'ctx._source.description += params.new_desc', \n",
    "                                              'params': {'new_desc' :desc + ' '}}})['result']\n",
    "                            if result != 'updated': print('failed to update description for :', entity)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            if i == 1000: break\n",
    "\n",
    "def populate_abstract_bytype(idx1,idx2, filename):\n",
    "    doc = {}\n",
    "    with open(filename, 'r', encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            if i > 1000: break\n",
    "            if i % 1000 == 0: print(i)\n",
    "            #skip header\n",
    "            if i == 0: continue                                   \n",
    "            #only consider dbpedia\n",
    "            if 'dbpedia.org' in line.lower():            \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line) \n",
    "                if entity is not None:\n",
    "                    #read types from existing entity index\n",
    "                    try:\n",
    "                        types = es.get(index=idx1, id=entity)['_source']['types']\n",
    "                        if types is not None:\n",
    "                            desc = resolve_abstract(line,ABSTRACTS)\n",
    "                            for typ in types:\n",
    "                               \n",
    "                                #update index or create index for each types found\n",
    "                                if es.exists(index=idx2, id=typ): \n",
    "                                    result = es.update(index=idx2, id=typ, body={\n",
    "                                        'script':{'source':'ctx._source.description += params.new_desc', \n",
    "                                            'params': {'new_desc' :desc + ' '}}})['result']\n",
    "                                    if result != 'updated': print('failed to update description for :', entity)\n",
    "                                else:\n",
    "                                    es.index(index=idx2, id=typ, body={'description':desc})\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_type'}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "#Create Entity based index\n",
    "#if es.indices.exists(INDEX_NAME_ENTITY):\n",
    "#    es.indices.delete(index=INDEX_NAME_ENTITY)\n",
    "#es.indices.create(index=INDEX_NAME_ENTITY, body=INDEX_SETTINGS_ENTITY)    \n",
    "\n",
    "#Create Type based index\n",
    "if es.indices.exists(INDEX_NAME_TYPE):\n",
    "    es.indices.delete(index=INDEX_NAME_TYPE)\n",
    "es.indices.create(index=INDEX_NAME_TYPE, body=INDEX_SETTINGS_TYPE)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate index\n",
    "\n",
    "There are 2 steps in populating index based on fields. This is due to fact that there are 3 different dataset files for 2 different fields, types and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Create entity based index , with types populated\n",
    "dirname = 'data'\n",
    "for file in type_files:\n",
    "    file = dirname + '/'+ file\n",
    "    populate_type(INDEX_NAME_ENTITY,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Create entity based index, with abstract populated\n",
    "dirname = 'data'\n",
    "for file in abstract_files:\n",
    "    file = dirname + '/'+ file\n",
    "    populate_abstract(INDEX_NAME_ENTITY, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "#3. Create type based index, with abstract populated\n",
    "dirname = 'data'\n",
    "for file in abstract_files:\n",
    "    file = dirname + '/'+ file\n",
    "    populate_abstract_bytype(INDEX_NAME_ENTITY,INDEX_NAME_TYPE, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[{'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:AdministrativeArea', '_score': 4.1482306}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:Region', '_score': 4.1482306}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:PopulatedPlace', '_score': 4.134841}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:Place', '_score': 4.129408}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:Location', '_score': 4.022934}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:owl#Thing', '_score': 3.8660688}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:BodyOfWater', '_score': 2.5585153}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:RiverBodyOfWater', '_score': 2.3980913}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:Stream', '_score': 2.3980913}, {'_index': 'nlp_type', '_type': '_doc', '_id': 'dbo:NaturalPlace', '_score': 2.370525}]\n"
     ]
    }
   ],
   "source": [
    "query = 'Who is the governor of alabama'\n",
    "#hits = es.search(index=INDEX_NAME_ENTITY, body={'query': {'match': {'description': query}}},\n",
    "#                               _source=True, size=1)\n",
    "#print(hits)\n",
    "#tv_1 = es.termvectors(index=INDEX_NAME_ENTITY, id='Crowe Clark Whitehill', fields='types')\n",
    "#print(tv_1)\n",
    "#res = es.get(index=INDEX_NAME_ENTITY, id=query)['_source']['types']\n",
    "#print(res)\n",
    "#res = es.get(index=INDEX_NAME_TYPE, id='dbo:Person')['_source']\n",
    "#print(res)\n",
    "hits = es.search(index=INDEX_NAME_TYPE, body={'query': {'match': {'description': query}}}, _source=False, size=10)\n",
    "print(hits['hits']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predic types for queries\n",
    "\n",
    "\n",
    "First score documents given the querie. Count the number of times a type occurs across the documents, dividing it by the sum of all type counts to get the type weight. For every type in a document, multiply the type weight with the document score and add to the types score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query):\n",
    "    tokens = es.indices.analyze(index=INDEX_NAME_TYPE, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def entity_centric_scorer(index_name, query, field='description', k=100):\n",
    "    es_query = {\n",
    "        \"query\":{\n",
    "            \"query_string\":{\n",
    "                \"query\": query,\n",
    "                \"default_field\": field\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matches = es.search(index=index_name, body=es_query, _source=True, size=k)['hits']['hits']\n",
    "\n",
    "    type_count = defaultdict(int)\n",
    "    for match in matches:\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_count[doc_type] += 1\n",
    "\n",
    "    type_weight = {}\n",
    "    for t, c in type_count.items():\n",
    "        type_weight[t] = 1/c\n",
    "\n",
    "    type_score = defaultdict(int)\n",
    "    for match in matches:\n",
    "        doc_id = match['_id']\n",
    "        doc_score = match['_score']\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_score[doc_type] += doc_score * type_weight[doc_type]\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_score.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def type_centric_scorer(index_name, q_terms, field='description', k=100):\n",
    "    type_scores = defaultdict(int)\n",
    "    for term in q_terms:\n",
    "        es_query = {\n",
    "            \"query\":{\n",
    "                \"query_string\":{\n",
    "                    \"query\": term,\n",
    "                    \"default_field\": field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        matches = es.search(index=index_name, body=es_query, _source=False, size=k)['hits']['hits']\n",
    "        for match in matches:\n",
    "            type_scores[match['_id']] += match['_score']\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_scores.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_scores = {}\n",
    "pattern = re.compile(r'[\\W_]+')\n",
    "for query in TRAIN_QUERY:\n",
    "    if query['category'] != 'resource':\n",
    "        continue\n",
    "    if query['question'] == None:\n",
    "        continue\n",
    "    q = pattern.sub(' ', query['question'])\n",
    "    ec_pred = entity_centric_scorer(INDEX_NAME_ENTITY, q)\n",
    "    q_terms = analyze_query(es, q)\n",
    "    tc_pred = type_centric_scorer(INDEX_NAME_TYPE, q_terms)\n",
    "    query_scores[query['id']] = {'entity_centric': ec_pred, 'type_centric': tc_pred, 'actual': query['type']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ID: dbpedia_14427\nEntity Centric: [('dbo:MusicalWork', 9.542570920000001), ('dbo:City', 8.740549), ('dbo:Language', 8.3763765), ('dbo:Work', 7.916609217647058), ('dbo:CreativeWork', 7.916609217647058), ('dbo:MusicAlbum', 7.8533206), ('dbo:ArchitecturalStructure', 7.66714895), ('dbo:Group', 7.656701066666666), ('dbo:AnatomicalStructure', 7.499036), ('dbo:Software', 7.494758)]\nType Centric: [('dbo:Person', 2.08592492), ('dbo:DUL.owl#NaturalPerson', 2.0530576700000003), ('dbo:owl#Thing', 1.9950488100000001), ('dbo:Agent', 1.9898861600000002), ('dbo:DUL.owl#Agent', 1.9898861600000002), ('dbo:Place', 1.66913342), ('dbo:CelestialBody', 1.65529437), ('dbo:Location', 1.61243721), ('dbo:Book', 1.55039071), ('dbo:Satellite', 1.5280187399999998)]\nActual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n\nID: dbpedia_3681\nEntity Centric: [('dbo:Organisation', 9.667582681034476), ('dbo:Organization', 9.52073445081967), ('dbo:DUL.owl#SocialPerson', 9.52073445081967), ('dbo:Agent', 9.059211415555557), ('dbo:DUL.owl#Agent', 9.059211415555557), ('dbo:owl#Thing', 8.907823842), ('dbo:Product', 8.769014), ('dbo:MeanOfTransportation', 8.769014), ('dbo:DUL.owl#DesignedArtifact', 8.769014), ('dbo:Food', 8.25833925)]\nType Centric: [('dbo:Organization', 3.37515368), ('dbo:DUL.owl#SocialPerson', 3.37515368), ('dbo:Organisation', 3.35802934), ('dbo:Agent', 3.26604387), ('dbo:DUL.owl#Agent', 3.26604387), ('dbo:owl#Thing', 2.9982000600000003), ('dbo:Company', 2.3636481), ('dbo:Product', 2.34076107), ('dbo:MeanOfTransportation', 2.0540404), ('dbo:DUL.owl#DesignedArtifact', 2.0540404)]\nActual: ['dbo:EducationalInstitution', 'dbo:Organisation', 'dbo:Agent']\n\nID: dbpedia_12020\nEntity Centric: [('dbo:Settlement', 10.871642722222223), ('dbo:AdministrativeArea', 10.643388529411766), ('dbo:Region', 10.643388529411766), ('dbo:PopulatedPlace', 10.599337649999999), ('dbo:Country', 10.573594661764705), ('dbo:Place', 10.483204248322153), ('dbo:Location', 10.470506406666665), ('dbo:owl#Thing', 10.202571804999998), ('dbo:City', 9.979554642857142), ('dbo:Organisation', 9.929254071428572)]\nType Centric: [('dbo:Place', 1.82237374), ('dbo:owl#Thing', 1.8074416199999999), ('dbo:Organisation', 1.8005126999999999), ('dbo:Location', 1.79915447), ('dbo:Organization', 1.79710758), ('dbo:DUL.owl#SocialPerson', 1.79710758), ('dbo:Agent', 1.7695222), ('dbo:DUL.owl#Agent', 1.7695222), ('dbo:PopulatedPlace', 1.76199213), ('dbo:Person', 1.6838140099999999)]\nActual: ['dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n\nID: dbpedia_10315\nEntity Centric: [('dbo:Product', 11.919854), ('dbo:MeanOfTransportation', 11.919854), ('dbo:DUL.owl#DesignedArtifact', 11.919854), ('dbo:Work', 10.596664400000002), ('dbo:CreativeWork', 10.596664400000002), ('dbo:DUL.owl#NaturalPerson', 9.435346000000001), ('dbo:Agent', 9.389724333333334), ('dbo:DUL.owl#Agent', 9.389724333333334), ('dbo:Person', 9.342599833333331), ('dbo:Organisation', 9.161616)]\nType Centric: [('dbo:WrittenWork', 2.27674957), ('dbo:Person', 2.212769745), ('dbo:Work', 2.21241434), ('dbo:CreativeWork', 2.21241434), ('dbo:DUL.owl#NaturalPerson', 2.184634704), ('dbo:owl#Thing', 2.1582813300000003), ('dbo:Agent', 2.142995316), ('dbo:DUL.owl#Agent', 2.142995316), ('dbo:Book', 2.03821108), ('dbo:Device', 1.307328)]\nActual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n\nID: dbpedia_1335\nEntity Centric: [('dbo:Movie', 14.020392), ('dbo:Work', 14.020392), ('dbo:CreativeWork', 14.020392), ('dbo:SocietalEvent', 9.238198320000002), ('dbo:DUL.owl#Event', 9.238198320000002), ('dbo:Event', 9.23819832), ('dbo:Person', 8.658078458620688), ('dbo:DUL.owl#NaturalPerson', 8.64125616), ('dbo:Politician', 8.128436), ('dbo:Agent', 8.064015854545453)]\nType Centric: [('dbo:Place', 0.8830338839999999), ('dbo:PopulatedPlace', 0.882107314), ('dbo:Location', 0.880410734), ('dbo:Country', 0.879993845), ('dbo:owl#Thing', 0.86934248), ('dbo:Organization', 0.863931486), ('dbo:DUL.owl#SocialPerson', 0.863931486), ('dbo:Organisation', 0.86118472), ('dbo:Language', 0.854852889), ('dbo:Settlement', 0.8389285449999999)]\nActual: ['dbo:Country', 'dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n\nID: dbpedia_6016\nEntity Centric: [('dbo:Product', 11.919854), ('dbo:MeanOfTransportation', 8.5145692), ('dbo:DUL.owl#DesignedArtifact', 8.5145692), ('dbo:Language', 8.166696533333333), ('dbo:PeriodicalLiterature', 7.7917085), ('dbo:Work', 7.608055853846153), ('dbo:CreativeWork', 7.608055853846153), ('dbo:Biomolecule', 7.463745749999999), ('dbo:WrittenWork', 7.4366165), ('dbo:Book', 7.3478435000000015)]\nType Centric: [('dbo:Person', 2.43582248), ('dbo:Agent', 2.3822623), ('dbo:DUL.owl#Agent', 2.3822623), ('dbo:DUL.owl#NaturalPerson', 2.33742553), ('dbo:owl#Thing', 2.24456946), ('dbo:Organization', 1.7893278700000002), ('dbo:DUL.owl#SocialPerson', 1.7893278700000002), ('dbo:Organisation', 1.78415662), ('dbo:Place', 1.36586092), ('dbo:PopulatedPlace', 1.34042894)]\nActual: ['dbo:AcademicSubject', 'dbo:TopicalConcept']\n\nID: dbpedia_3432\nEntity Centric: [('dbo:Country', 14.141824), ('dbo:Cleric', 11.612139333333333), ('dbo:Book', 11.538778166666667), ('dbo:DUL.owl#TimeInterval', 11.006678999999998), ('dbo:WrittenWork', 10.872206888888886), ('dbo:ChemicalSubstance', 10.506901142857142), ('dbo:DUL.owl#ChemicalObject', 10.506901142857142), ('dbo:Person', 10.479203163636363), ('dbo:DUL.owl#NaturalPerson', 10.352644849999999), ('dbo:City', 10.185737)]\nType Centric: [('dbo:Place', 5.6400457), ('dbo:Location', 5.35032763), ('dbo:owl#Thing', 4.882546649999999), ('dbo:Agent', 4.385010019999999), ('dbo:DUL.owl#Agent', 4.385010019999999), ('dbo:Organization', 4.27270027), ('dbo:DUL.owl#SocialPerson', 4.27270027), ('dbo:Person', 2.93143569), ('dbo:DUL.owl#NaturalPerson', 2.79196727), ('dbo:CelestialBody', 2.34218514)]\nActual: ['dbo:Name']\n\nID: dbpedia_16006\nEntity Centric: [('dbo:Event', 8.202938600000001), ('dbo:SocietalEvent', 8.2029386), ('dbo:DUL.owl#Event', 8.2029386), ('dbo:Book', 7.566591276923081), ('dbo:WrittenWork', 7.566591276923076), ('dbo:Work', 7.552625928571427), ('dbo:CreativeWork', 7.552625928571427), ('dbo:DUL.owl#NaturalPerson', 7.414804608474574), ('dbo:Person', 7.376527992814374), ('dbo:Agent', 7.322555637313432)]\nType Centric: [('dbo:Book', 5.68278162), ('dbo:Work', 5.30019086), ('dbo:CreativeWork', 5.30019086), ('dbo:WrittenWork', 5.18659418), ('dbo:owl#Thing', 3.20028775), ('dbo:Person', 1.94384224), ('dbo:DUL.owl#NaturalPerson', 1.7714670399999999), ('dbo:Agent', 1.67038873), ('dbo:DUL.owl#Agent', 1.67038873), ('dbo:Organization', 0.69464384)]\nActual: ['dbo:Organisation', 'dbo:Agent']\n\nID: dbpedia_278\nEntity Centric: [('dbo:MusicalWork', 16.8703015), ('dbo:Movie', 16.77602303571428), ('dbo:Work', 16.70285012903226), ('dbo:CreativeWork', 16.70285012903226), ('dbo:owl#Thing', 15.795442635), ('dbo:DUL.owl#NaturalPerson', 15.428458097014923), ('dbo:Agent', 15.398610977941175), ('dbo:DUL.owl#Agent', 15.398610977941175), ('dbo:Person', 15.371262366197183), ('dbo:Artist', 15.01831425)]\nType Centric: [('dbo:Agent', 3.5169538499999997), ('dbo:DUL.owl#Agent', 3.5169538499999997), ('dbo:owl#Thing', 3.47828303), ('dbo:MusicGroup', 3.45413929), ('dbo:Group', 3.4507950099999998), ('dbo:Organisation', 3.43038565), ('dbo:Organization', 3.39355325), ('dbo:DUL.owl#SocialPerson', 3.39355325), ('dbo:Person', 3.07372245), ('dbo:DUL.owl#NaturalPerson', 2.94369605)]\nActual: ['dbo:Person', 'dbo:Agent']\n\nID: dbpedia_7661\nEntity Centric: [('dbo:SportsTeam', 10.808322), ('dbo:Settlement', 8.173607), ('dbo:SportsLeague', 7.9508657), ('dbo:School', 7.686251), ('dbo:EducationalInstitution', 7.686251), ('dbo:EducationalOrganization', 7.686251), ('dbo:Politician', 7.265807), ('dbo:Cleric', 5.686584325), ('dbo:Organisation', 5.5950017249999995), ('dbo:Organization', 5.5950017249999995)]\nType Centric: [('dbo:Person', 2.99949126), ('dbo:DUL.owl#NaturalPerson', 2.76618165), ('dbo:Agent', 2.6035968), ('dbo:DUL.owl#Agent', 2.6035968), ('dbo:owl#Thing', 2.1129423), ('dbo:Cleric', 0.91939908), ('dbo:Place', 0.2580425), ('dbo:Organization', 0.25799263), ('dbo:DUL.owl#SocialPerson', 0.25799263), ('dbo:Organisation', 0.2569571)]\nActual: ['dbo:Person', 'dbo:Agent']\n\nID: dbpedia_11196\nEntity Centric: [('dbo:Country', 9.884888499999999), ('dbo:Settlement', 9.703798), ('dbo:SocietalEvent', 9.4259205), ('dbo:Event', 9.4259205), ('dbo:DUL.owl#Event', 9.4259205), ('dbo:DUL.owl#NaturalPerson', 9.334907108860754), ('dbo:Person', 9.318882661538467), ('dbo:Agent', 9.27819171627907), ('dbo:DUL.owl#Agent', 9.27819171627907), ('dbo:Cleric', 9.235585440000001)]\nType Centric: [('dbo:Person', 3.37674227), ('dbo:DUL.owl#NaturalPerson', 3.2942983200000002), ('dbo:Agent', 3.2421927900000003), ('dbo:DUL.owl#Agent', 3.2421927900000003), ('dbo:owl#Thing', 3.06604907), ('dbo:Place', 2.11070795), ('dbo:Location', 2.05342852), ('dbo:PopulatedPlace', 1.72862847), ('dbo:Organisation', 1.4874861799999999), ('dbo:Work', 1.4874497999999998)]\nActual: ['dbo:WrittenWork', 'dbo:Work']\n\n"
     ]
    }
   ],
   "source": [
    "for i, (key, val) in enumerate(query_scores.items()):\n",
    "    print(\"ID: {}\".format(key))\n",
    "    print(\"Entity Centric: {}\".format(val['entity_centric'][:10]))\n",
    "    print(\"Type Centric: {}\".format(val['type_centric'][:10]))\n",
    "    print(\"Actual: {}\\n\".format(val['actual']))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(predicted, ground_truth):\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def recall(predicted, ground_truth):\n",
    "    if len(ground_truth) == 0: return 0\n",
    "    correct = 0\n",
    "    for t in ground_truth:\n",
    "        if t in predicted:\n",
    "            correct += 1\n",
    "    return correct/len(ground_truth)\n",
    "\n",
    "\n",
    "def avg_precision(predicted, ground_truth):\n",
    "    precision = []\n",
    "    correct = 0\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            correct += 1\n",
    "            precision.append(correct/(i+1))\n",
    "\n",
    "    if len(precision) == 0: return 0\n",
    "    return sum(precision)/len(precision)\n",
    "\n",
    "def mean(predictions, ground_truths, func):\n",
    "    stats = []\n",
    "    for i in range(len(predictions)):\n",
    "        stats.append(func(predictions[i], ground_truths[i]))\n",
    "    return sum(stats)/len(stats) if len(stats) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Entity Centric\nRecall: 0.7133303131635906\nMRR: 0.18212586849916343\nMean Precision: 0.18212586849916343\n\nType Centric\nRecall: 0.703204389728525\nMRR: 0.4134076986356575\nMean Precision: 0.4134076986356575 \n\n"
     ]
    }
   ],
   "source": [
    "ec_preds = []\n",
    "tc_preds = []\n",
    "ground_truths = []\n",
    "for _, val in query_scores.items():\n",
    "    ec_preds.append([x[0] for x in val['entity_centric']])\n",
    "    tc_preds.append([x[0] for x in val['type_centric']])\n",
    "    ground_truths.append(val['actual'])\n",
    "\n",
    "m_recall_ec = mean(ec_preds, ground_truths ,recall)\n",
    "mrr_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "m_recall_tc = mean(tc_preds, ground_truths ,recall)\n",
    "mrr_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "print(\"Entity Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_ec))\n",
    "print(\"MRR: {}\".format(mrr_ec))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_ec))\n",
    "\n",
    "print(\"Type Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_tc))\n",
    "print(\"MRR: {}\".format(mrr_tc))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_tc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}