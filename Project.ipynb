{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "import elasticsearch\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pytest\n",
    "import random\n",
    "import requests\n",
    "import tarfile\n",
    "import json\n",
    "import bz2file\n",
    "import bz2\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from bz2 import BZ2File as bzopen\n",
    "from elasticsearch import Elasticsearch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Query from Github\n",
    "\n",
    "Download query from github and split into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14056\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/datasets/DBpedia/'\n",
    "file = 'smarttask_dbpedia_train.json'\n",
    "url = url + file\n",
    "queries=requests.get(url).json()\n",
    "\n",
    "\n",
    "random.seed(a=1234567)\n",
    "TRAIN_SIZE = int(len(queries) * 0.8)\n",
    "\n",
    "TRAIN_QUERY = queries[:TRAIN_SIZE]\n",
    "TEST_QUERY = queries[TRAIN_SIZE:]\n",
    "\n",
    "print(len(TRAIN_QUERY))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file from DBPedia NLP dataset\n",
    "\n",
    "Check whether dataset files (bz2) are available in data folder and download bz2 file if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburl= 'http://downloads.dbpedia.org/2016-10/core-i18n/en/'\n",
    "#dbfiles = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2','instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "#cwd = os.getcwd()  \n",
    "dbfiles = ['long_abstracts_en.ttl.bz2','instance_types_transitive_en.ttl.bz2']\n",
    "dirname = 'data'\n",
    "\n",
    "for dbfile in dbfiles:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname) \n",
    "        \n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "        # Download file\n",
    "        url = dburl + dbfile\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "    else:\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Index configuration \n",
    "\n",
    "For each of the fields, store the term vectors. These should be stored in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "INDEX_NAME = 'nlp_entity'\n",
    "\n",
    "INDEX_SETTINGS = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                #'names': {\n",
    "                #    'type': 'text',\n",
    "                #    'term_vector': 'yes',\n",
    "                #    'analyzer': 'english'\n",
    "                #},\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'types': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                #'catch_all': {\n",
    "                #    'type': 'text',\n",
    "                #    'term_vector': 'yes',\n",
    "                #    'analyzer': 'english'\n",
    "                #},\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS = '<http://dbpedia.org/ontology/abstract>'\n",
    "COMMENTS = '<http://www.w3.org/2000/01/rdf-schema#comment>'\n",
    "TYPES = '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
    "#abstract_files = ['long_abstracts_en.ttl.bz2','short_abstracts_en.ttl.bz2']\n",
    "#type_files = ['instance_types_sdtyped_dbo_en.ttl.bz2','instance_types_en.ttl.bz2']\n",
    "abstract_files = ['long_abstracts_en.ttl.bz2']\n",
    "type_files = ['instance_types_transitive_en.ttl.bz2']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abstract(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    text = text[1].replace('\"', ' ')\n",
    "    text = text.replace(' @en .','')\n",
    "    return text\n",
    "\n",
    "def resolve_entity(text):\n",
    "    name = text.split('>')[0].replace('_', ' ')\n",
    "    name = name.split('/')[-1]\n",
    "    return name\n",
    "\n",
    "def resolve_types(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    types = text[1]    \n",
    "    types = types[types.rfind('/')+1:types.rfind('>')]\n",
    "    return 'dbo: '+types\n",
    "\n",
    "def populate_type(idx, filename):\n",
    "    doc = {}\n",
    "    #cwd = os.getcwd()\n",
    "    #myfile= os.path.join(cwd, dbfile)\n",
    "    myfile = 'data' + '/' +  filename\n",
    "    with bzopen(myfile, \"r\") as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 0: continue                #skip header\n",
    "            line = line.rstrip().decode('utf-8')\n",
    "            entity = resolve_entity(line)\n",
    "            types = []\n",
    "            if entity is not None:\n",
    "                if TYPES in line:                         #update index if it already exist if not then create new one    \n",
    "                    types.append(resolve_types(line,TYPES))\n",
    "                    if es.exists(index=idx, id=entity):\n",
    "                        result = es.update(index=idx, id=entity, body={\n",
    "                            'script':{'source':'ctx._source.types.add(params.new_type)', 'params':\n",
    "                                          {'new_type' : types}}})['result']\n",
    "                        #result = es.update(index=idx, id=entity, body={\n",
    "                        #    'script':{'source':'if(ctx._source.types.contains(params.new_type)){ctx._source.types=ctx._source.types} else {ctx._source.types.add(params.new_type)}', \n",
    "                        #    'params': {'new_type' : types}}})['result']\n",
    "\n",
    "                    else:\n",
    "                        es.index(index=idx, id=entity, body={'types':[],'description':''})\n",
    "                        result = es.update(index=idx, id=entity, body={\n",
    "                            'script':{'source':'ctx._source.types.add(params.new_type)', 'params':\n",
    "                                          {'new_type' : types}}})['result']\n",
    "                        if result != 'updated': print('failed to update type for :', entity)\n",
    "\n",
    "            if i == 100: break\n",
    "\n",
    "def populate_abstract(idx, filename):\n",
    "    doc = {}\n",
    "    #cwd = os.getcwd()\n",
    "    #myfile= os.path.join(cwd, filename)\n",
    "    myfile = 'data' + '/'  + filename\n",
    "    with bzopen(myfile, \"r\") as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 0: continue                      #skip header\n",
    "            line = line.rstrip().decode('utf-8')\n",
    "            entity = resolve_entity(line)\n",
    "            if entity is not None:\n",
    "                if ABSTRACTS in line:     \n",
    "                    desc = resolve_abstract(line,ABSTRACTS)\n",
    "                elif COMMENTS in line:\n",
    "                    desc = resolve_abstract(line,COMMENTS)\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "                if es.exists(index=idx, id=entity):    #only update index if already exist \n",
    "                    result = es.update(index=idx, id=entity, body={\n",
    "                            'script':{'source':'ctx._source.description += params.new_desc', \n",
    "                                      'params': {'new_desc' :desc + ' '}}})['result']\n",
    "\n",
    "                    #result =es.update(index=idx, id=entity, body={\n",
    "                    #        'script':{'source':'if(ctx._source.description.contains(params.new_desc)){ctx._source.description=ctx._source.description} else {ctx._source.description += params.new_desc}', \n",
    "                    #        'params': {'new_desc' : ' ' + desc}}})['result']\n",
    "                    if result != 'updated': print('failed to update description for :', entity)\n",
    "                    \n",
    "                #else:\n",
    "                #    es.index(index=idx, id=entity, body={'types':[],'description':desc})\n",
    "\n",
    "            if i == 100: break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_entity'}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME):\n",
    "    es.indices.delete(index=INDEX_NAME)\n",
    "es.indices.create(index=INDEX_NAME, body=INDEX_SETTINGS)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate index\n",
    "\n",
    "There are 2 steps in populating index based on fields. This is due to fact that there are 3 different dataset files for 2 different fields, types and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in type_files:\n",
    "    populate_type(INDEX_NAME,file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in abstract_files:\n",
    "    populate_abstract(INDEX_NAME, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 1, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 1, 'relation': 'eq'}, 'max_score': 4.172229, 'hits': [{'_index': 'nlp_entity', '_type': '_doc', '_id': 'Alabama', '_score': 4.172229, '_source': {'types': [['dbo: AdministrativeArea'], ['dbo: Q3455524'], ['dbo: Region'], ['dbo: PopulatedPlace'], ['dbo: Place'], ['dbo: Place'], ['dbo: Location'], ['dbo: owl#Thing']], 'description': \"  Alabama (/ˌæləˈbæmə/) is a state in the southeastern region of the United States. It is bordered by Tennessee to the north, Georgia to the east, Florida and the Gulf of Mexico to the south, and Mississippi to the west. Alabama is the 30th-most extensive and the 24th-most populous of the 50 United States. At 1,300 miles (2,100 km), Alabama has one of the nation's longest navigable inland waterways. From the American Civil War until World War II, Alabama, like many states in the South, suffered economic hardship, in part because of continued dependence on agriculture. Despite the growth of major industries and urban centers, white rural interests dominated the state legislature from 1901 to the 1960s, as it did not regularly reapportion the legislature from 1901 to 1961; urban interests and African Americans were markedly under-represented. African Americans and poor whites were essentially disenfranchised altogether by the state constitution of 1901, a status that continued into the mid-1960s before being alleviated by federal legislation. Exclusion of minorities continued under at-large voting systems in most counties; some changes were made through a series of omnibus court cases in the late 1980s to establish different electoral systems. Following World War II, Alabama experienced growth as the state's economy changed from one primarily based on agriculture to one with diversified interests. The power of the Solid South in Congress gained the establishment or expansion of multiple United States Armed Forces installations, which helped bridge the gap between an agricultural and industrial economy during the mid-20th century. The state economy in the 21st century is based on management, automotive, finance, manufacturing, aerospace, mineral extraction, healthcare, education, retail, and technology. Alabama is nicknamed the Yellowhammer State, after the state bird. Alabama is also known as the \\\\ Heart of Dixie\\\\  and the Cotton State. The state tree is the longleaf pine, and the state flower is the camellia. Alabama's capital is Montgomery. The largest city by population is Birmingham, which has long been the most industrialized city, and largest city by land area is Huntsville. The oldest city is Mobile, founded by French colonists in 1702 as the capital of French Louisiana. \"}}]}}\n",
      "{'_index': 'nlp_entity', '_type': '_doc', '_id': 'Alabama', '_version': 10, 'found': True, 'took': 0, 'term_vectors': {'types': {'field_statistics': {'sum_doc_freq': 671, 'doc_count': 113, 'sum_ttf': 1257}, 'terms': {'administrativearea': {'term_freq': 1}, 'dbo': {'term_freq': 8}, 'locat': {'term_freq': 1}, 'owl': {'term_freq': 1}, 'place': {'term_freq': 2}, 'populatedplac': {'term_freq': 1}, 'q3455524': {'term_freq': 1}, 'region': {'term_freq': 1}, 'thing': {'term_freq': 1}}}}}\n"
     ]
    }
   ],
   "source": [
    "query = 'Alabama'\n",
    "hits = es.search(index=INDEX_NAME, body={'query': {'match': {'description': query}}},\n",
    "                               _source=True, size=10)\n",
    "print(hits)\n",
    "tv_1 = es.termvectors(index=INDEX_NAME, id='Alabama', fields='types')\n",
    "print(tv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
