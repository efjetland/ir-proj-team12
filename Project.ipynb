{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import bz2\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet,stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query training dataset from Github\n",
    "\n",
    "Download query from github and split into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training query:  14056\n",
      "number of resource type query in training: 7655\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/datasets/DBpedia/'\n",
    "file = 'smarttask_dbpedia_train.json'\n",
    "url = url + file\n",
    "queries=requests.get(url).json()\n",
    "\n",
    "\n",
    "random.seed(a=1234567)\n",
    "TRAIN_SIZE = int(len(queries) * 0.8)\n",
    "\n",
    "TRAIN_QUERY = queries[:TRAIN_SIZE]\n",
    "TEST_QUERY = queries[TRAIN_SIZE:]\n",
    "print('number of training query: ',len(TRAIN_QUERY))\n",
    "\n",
    "TRAIN_QUERY=  [q for q in TRAIN_QUERY if q['category']=='resource']\n",
    "print('number of resource type query in training:',len(TRAIN_QUERY))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query stats\n",
    "#query = Counter([q['category'] for q in TRAIN_QUERY])\n",
    "#print(query)\n",
    "\n",
    "#resource_query = [q['type'] for q in TRAIN_QUERY if q['category']=='resource']\n",
    "#query_type = Counter(sum(resource_query,[]))\n",
    "\n",
    "#print(resource_query)\n",
    "#c0=0\n",
    "#c1=0\n",
    " \n",
    "#for q in resource_query:\n",
    "#    if len(resource_query[q])>1: c1 += 1\n",
    "#    else: c0 += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file from DBPedia NLP dataset\n",
    "\n",
    "Check whether dataset files (bz2) are available in data folder and download bz2 file if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburl= 'http://downloads.dbpedia.org/2016-10/core-i18n/en/'\n",
    "dbfiles = ['long_abstracts_en.ttl.bz2','instance_types_transitive_en.ttl.bz2']\n",
    "type_file = 'dbpedia_types.tsv'\n",
    "dirname = 'data'\n",
    "\n",
    "for dbfile in dbfiles:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname) \n",
    "    \n",
    "    # Download file\n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "        url = dburl + dbfile\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "   \n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    #extract\n",
    "    if not os.path.exists(dirname + '/' + dbfile[:-4]):\n",
    "        filepath= dirname + '/'+ dbfile\n",
    "        newfilepath = filepath[:-4] \n",
    "        with open(filepath, 'rb') as source, open(newfilepath, 'wb') as dest:\n",
    "            dest.write(bz2.decompress(source.read()))\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/evaluation/dbpedia/dbpedia_types.tsv'\n",
    "if not os.path.exists(dirname + '/' + type_file):\n",
    "    r = requests.get(url)\n",
    "    with open(dirname + '/'+ type_file, 'wb') as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Index configuration \n",
    "\n",
    "For each of the fields, store the term vectors. These should be stored in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "INDEX_NAME_ENTITY = 'nlp_entity'\n",
    "\n",
    "INDEX_SETTINGS_ENTITY = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'types': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "INDEX_NAME_TYPE = 'nlp_type'\n",
    "\n",
    "INDEX_SETTINGS_TYPE = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'question': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS = '<http://dbpedia.org/ontology/abstract>'\n",
    "COMMENTS = '<http://www.w3.org/2000/01/rdf-schema#comment>'\n",
    "TYPES = '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
    "abstract_files = ['long_abstracts_en.ttl']\n",
    "type_files = ['instance_types_transitive_en.ttl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abstract(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    text = text[1].replace('\"', '')\n",
    "    text = text.replace('@en .','')\n",
    "    return text\n",
    "\n",
    "def resolve_entity(text):\n",
    "    name = text.split('>')[0].replace('_', ' ')\n",
    "    name = name.split('/')[-1]\n",
    "    return name\n",
    "\n",
    "def resolve_types(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    types = text[1]    \n",
    "    types = types[types.rfind('/')+1:types.rfind('>')]\n",
    "    return ['dbo:'+types]\n",
    "\n",
    "def populate_type(filename,entity_doc):\n",
    "    #doc structure = {doc: {types: , abstract:}}\n",
    "    with open(filename,'r',encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            #skip header\n",
    "            \n",
    "            if i == 1000000: break\n",
    "            if i == 0: continue        \n",
    "            #only consider dbpedia\n",
    "            if 'dbpedia.org/ontology' in line.lower(): \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line)\n",
    "                if entity is not None:\n",
    "                    if TYPES in line:   \n",
    "                        types=resolve_types(line,TYPES) \n",
    "                        #add value or create new id in dictionary\n",
    "                        if entity in entity_doc:\n",
    "                            entity_doc[entity]['types']+= types\n",
    "                        else:\n",
    "                            entity_doc[entity]={'types':types}\n",
    "                        \n",
    "            \n",
    "            \n",
    "    return entity_doc\n",
    "\n",
    "def populate_abstract(filename, entity_doc):\n",
    "    #doc structure = {doc: {types: , abstract:}}\n",
    "    with open(filename, 'r', encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 1000000: break\n",
    "            #skip header\n",
    "            if i == 0: continue      \n",
    "            #only consider dbpedia, in english\n",
    "            if 'dbpedia.org/ontology' in line.lower() and 'en' in line.lower():            \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line) \n",
    "                if entity is not None:\n",
    "                    if ABSTRACTS in line:     \n",
    "                        desc = resolve_abstract(line,ABSTRACTS)\n",
    "                        if entity in entity_doc:\n",
    "                            if entity_doc[entity]['types'] is not None:\n",
    "                                entity_doc[entity]['description']= entity_doc[entity].get('description','') + desc\n",
    "                            else:                             \n",
    "                                continue\n",
    "                        else:\n",
    "                            continue\n",
    "            \n",
    "    return entity_doc\n",
    "\n",
    "def prepare_doc(type_files,abstract_files):\n",
    "    entity_doc={}\n",
    "    type_doc = {}\n",
    "    dirname ='data'\n",
    "    for file in type_files:\n",
    "        file = dirname + '/'+ file\n",
    "        entity_doc = populate_type(file,entity_doc)\n",
    "    print('type files are read')\n",
    "    for file in abstract_files:\n",
    "        file = dirname + '/'+ file\n",
    "        entity_doc = populate_abstract( file,entity_doc)\n",
    "    print('abstract files are read')\n",
    "    print('done populating entity doc')\n",
    "    #build type_doc from entity_doc\n",
    "    for entity in entity_doc:\n",
    "        types = entity_doc[entity]['types']\n",
    "        if types is not None:\n",
    "            for typ in types:\n",
    "                if typ in type_doc:\n",
    "                    type_doc[typ]['description']= type_doc[typ].get('description','') + entity_doc[entity].get('description','')\n",
    "                else:\n",
    "                    type_doc[typ] = {'description': entity_doc[entity].get('description','')}\n",
    "    \n",
    "    print('done populating type doc')\n",
    "    \n",
    "    #export to json\n",
    "    jsn = json.dumps(entity_doc)\n",
    "    f = open(\"entity_doc.json\",\"w\")\n",
    "    f.write(jsn)\n",
    "    f.close()\n",
    "    \n",
    "    jsn = json.dumps(type_doc)\n",
    "    f = open(\"type_doc.json\",\"w\")\n",
    "    f.write(jsn)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    return entity_doc, type_doc\n",
    "        \n",
    "\n",
    "def create_index_bulk(INDEX_NAME_ENTITY,INDEX_NAME_TYPE,type_files,abstract_files):   \n",
    "    entity_docs, type_docs = prepare_doc(type_files,abstract_files)\n",
    "    action_entity = [\n",
    "    {\n",
    "    '_id' : ids,\n",
    "    \"_source\" : entity_docs[ids]\n",
    "    }\n",
    "    for ids in entity_docs\n",
    "    ]\n",
    "    bulk = helpers.bulk(es,action_entity, index = INDEX_NAME_ENTITY, chunk_size=1000, request_timeout=200)\n",
    "        \n",
    "    print('entity doc are indexed')\n",
    "    action_type = [\n",
    "    {\n",
    "    '_id' : ids,\n",
    "    \"_source\" : type_docs[ids]\n",
    "    }\n",
    "    for ids in type_docs\n",
    "    ]\n",
    "    bulk = helpers.bulk(es,action_type, index = INDEX_NAME_TYPE, chunk_size=1000, request_timeout=200)\n",
    "    print('type doc are indexed')\n",
    "\n",
    "def populate_question(queries, INDEX_NAME_TYPE):\n",
    "    type_dict = defaultdict(str)\n",
    "    for query in queries:\n",
    "        if query['question'] == None:\n",
    "            continue\n",
    "        for t in query['type']:\n",
    "            type_dict[t] += query['question']\n",
    "    for t, q in type_dict.items():\n",
    "        if es.exists(index=INDEX_NAME_TYPE, id=t):\n",
    "            es.update(index=INDEX_NAME_TYPE, id=t, body={\"doc\":{\"question\": type_dict[t]}})\n",
    "        else:\n",
    "            es.index(index=INDEX_NAME_TYPE, id=t, body={\"doc\":{\"question\": type_dict[t]}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate index\n",
    "\n",
    "Declare two new indexes, one for entity centric model and the other for type centric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_entity'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME_ENTITY):\n",
    "    es.indices.delete(index=INDEX_NAME_ENTITY)\n",
    "es.indices.create(index=INDEX_NAME_ENTITY, body=INDEX_SETTINGS_ENTITY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_type'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME_TYPE):\n",
    "    es.indices.delete(index=INDEX_NAME_TYPE)\n",
    "es.indices.create(index=INDEX_NAME_TYPE, body=INDEX_SETTINGS_TYPE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type files are read\n",
      "abstract files are read\n",
      "done populating entity doc\n",
      "done populating type doc\n",
      "entity doc are indexed\n",
      "type doc are indexed\n"
     ]
    }
   ],
   "source": [
    "create_index_bulk(INDEX_NAME_ENTITY,INDEX_NAME_TYPE,type_files,abstract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_question(TRAIN_QUERY, INDEX_NAME_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24848932\n",
      "['dbo:Region', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "[{'epoch': '1604857588', 'timestamp': '17:46:28', 'count': '120104'}]\n",
      "[{'epoch': '1604857588', 'timestamp': '17:46:28', 'count': '97'}]\n"
     ]
    }
   ],
   "source": [
    "#check indexes\n",
    "res = es.get(index='nlp_type', id='dbo:Place')['_source']['description']\n",
    "print(len(res))\n",
    "res = es.get(index='nlp_entity', id='Alabama')['_source']['types']\n",
    "print(res)\n",
    "count = es.cat.count('nlp_entity', params={\"format\": \"json\"})\n",
    "print(count)\n",
    "count = es.cat.count('nlp_type', params={\"format\": \"json\"})\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predic types for queries\n",
    "\n",
    "\n",
    "First score documents given the querie. Count the number of times a type occurs across the documents, dividing it by the sum of all type counts to get the type weight. For every type in a document, multiply the type weight with the document score and add to the types score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query):\n",
    "    tokens = es.indices.analyze(index=INDEX_NAME_TYPE, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def entity_centric_scorer(index_name, query, field='description', k=100):\n",
    "    es_query = {\n",
    "        \"query\":{\n",
    "            \"query_string\":{\n",
    "                \"query\": query,\n",
    "                \"default_field\": field\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matches = es.search(index=index_name, body=es_query, _source=True, size=k)['hits']['hits']\n",
    "\n",
    "    type_count = defaultdict(int)\n",
    "    for match in matches:\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_count[doc_type] += 1\n",
    "\n",
    "    type_weight = {}\n",
    "    for t, c in type_count.items():\n",
    "        type_weight[t] = 1/c\n",
    "\n",
    "    type_score = defaultdict(int)\n",
    "    for match in matches:\n",
    "        doc_id = match['_id']\n",
    "        doc_score = match['_score']\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_score[doc_type] += doc_score * type_weight[doc_type]\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_score.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def type_centric_scorer(index_name, q_terms, field='description', k=100):\n",
    "    type_scores = defaultdict(int)\n",
    "    for term in q_terms:\n",
    "        es_query = {\n",
    "            \"query\":{\n",
    "                \"query_string\":{\n",
    "                    \"query\": term,\n",
    "                    \"default_field\": field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        matches = es.search(index=index_name, body=es_query, _source=False, size=k)['hits']['hits']\n",
    "        for match in matches:\n",
    "            type_scores[match['_id']] += match['_score']\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_scores.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    376\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-064936251f60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mec_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mentity_centric_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINDEX_NAME_ENTITY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mq_terms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyze_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtc_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_centric_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mINDEX_NAME_TYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_terms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mquery_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'entity_centric'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mec_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'type_centric'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtc_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'actual'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-cb4f744bc8dc>\u001b[0m in \u001b[0;36mtype_centric_scorer\u001b[1;34m(index_name, q_terms, field, k)\u001b[0m\n\u001b[0;32m     50\u001b[0m         }\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mes_query\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_source\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'hits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mtype_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    150\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\client\\__init__.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, body, index, doc_type, params, headers)\u001b[0m\n\u001b[0;32m   1615\u001b[0m             \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1616\u001b[0m             \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1617\u001b[1;33m             \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1618\u001b[0m         )\n\u001b[0;32m   1619\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[0;32m    363\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m                 )\n\u001b[0;32m    367\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\elasticsearch\\connection\\http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[1;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    245\u001b[0m             response = self.pool.urlopen(\n\u001b[1;32m--> 246\u001b[1;33m                 \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mRetry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m             )\n\u001b[0;32m    248\u001b[0m             \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    601\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1319\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1322\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query_scores = {}\n",
    "pattern = re.compile(r'[\\W_]+')\n",
    "for query in TRAIN_QUERY:\n",
    "    if query['category'] != 'resource':\n",
    "        continue\n",
    "    if query['question'] == None:\n",
    "        continue\n",
    "    q = pattern.sub(' ', query['question'])\n",
    "    ec_pred = entity_centric_scorer(INDEX_NAME_ENTITY, q)\n",
    "    q_terms = analyze_query(es, q)\n",
    "    tc_pred = type_centric_scorer(INDEX_NAME_TYPE, q_terms)\n",
    "    query_scores[query['id']] = {'entity_centric': ec_pred, 'type_centric': tc_pred, 'actual': query['type']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: dbpedia_14427\n",
      "Entity Centric: [('dbo:Group', 15.700501499999998), ('dbo:Cartoon', 14.047559), ('dbo:Organisation', 13.0069865), ('dbo:RouteOfTransportation', 12.868805), ('dbo:Infrastructure', 12.868805), ('dbo:MusicalWork', 10.383891444444444), ('dbo:Agent', 10.352069909090911), ('dbo:Work', 10.320312022222225), ('dbo:WrittenWork', 10.244019), ('dbo:Software', 10.227817)]\n",
      "Type Centric: [('dbo:MusicalWork', 6.932363639999999), ('dbo:Work', 6.85984862), ('dbo:WrittenWork', 6.844822239999999), ('dbo:Group', 6.74023693), ('dbo:Agent', 6.6516779), ('dbo:ArchitecturalStructure', 6.5886717), ('dbo:Wikidata:Q11424', 6.58418887), ('dbo:Person', 6.559901190000001), ('dbo:Artist', 6.54073076), ('dbo:Location', 6.474589539999999)]\n",
      "Actual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n",
      "\n",
      "ID: dbpedia_3681\n",
      "Entity Centric: [('dbo:Food', 13.540861), ('dbo:Artist', 12.90148), ('dbo:Company', 11.979504888888886), ('dbo:Agent', 11.927975352631583), ('dbo:Person', 11.912545999999999), ('dbo:Organisation', 11.718885999999998), ('dbo:Broadcaster', 11.518802), ('dbo:PopulatedPlace', 10.90964825), ('dbo:Place', 10.90964825), ('dbo:Location', 10.90964825)]\n",
      "Type Centric: [('dbo:Agent', 9.28586611), ('dbo:Company', 6.81826095), ('dbo:Work', 6.766858919999999), ('dbo:Organisation', 6.74814782), ('dbo:Person', 6.666759020000001), ('dbo:MusicalWork', 6.58095596), ('dbo:WrittenWork', 6.5111526), ('dbo:Group', 6.41623298), ('dbo:Wikidata:Q11424', 6.379384379999999), ('dbo:PeriodicalLiterature', 6.2533329)]\n",
      "Actual: ['dbo:EducationalInstitution', 'dbo:Organisation', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_12020\n",
      "Entity Centric: [('dbo:ArchitecturalStructure', 12.466058), ('dbo:Organisation', 12.3633530625), ('dbo:Place', 12.340116870967748), ('dbo:Location', 12.340116870967748), ('dbo:PopulatedPlace', 12.335918833333333), ('dbo:Settlement', 12.254845277777775), ('dbo:Agent', 12.055326434210524), ('dbo:Politician', 11.999879857142856), ('dbo:Person', 11.937496068965515), ('dbo:Region', 11.85902769047619)]\n",
      "Type Centric: [('dbo:Region', 7.92786691), ('dbo:Politician', 7.9090586), ('dbo:PopulatedPlace', 7.761287729999999), ('dbo:Location', 7.6931470299999996), ('dbo:Place', 7.693122939999999), ('dbo:Settlement', 7.66724338), ('dbo:Person', 7.58539858), ('dbo:Agent', 7.50045232), ('dbo:Organisation', 7.22019414), ('dbo:ArchitecturalStructure', 6.73481116)]\n",
      "Actual: ['dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "\n",
      "ID: dbpedia_10315\n",
      "Entity Centric: [('dbo:WrittenWork', 16.777676), ('dbo:Cartoon', 15.164494), ('dbo:Work', 15.124240666666665), ('dbo:Language', 14.219619), ('dbo:Animal', 13.531187), ('dbo:Eukaryote', 13.531187), ('dbo:Species', 13.531187), ('dbo:TimePeriod', 13.467945414893615), ('dbo:PopulatedPlace', 13.373524), ('dbo:Place', 13.373524)]\n",
      "Type Centric: [('dbo:Work', 7.142646150000001), ('dbo:MusicalWork', 7.029408179999999), ('dbo:WrittenWork', 7.015293870000001), ('dbo:Wikidata:Q11424', 6.96027084), ('dbo:Artist', 6.95725148), ('dbo:Agent', 6.83414707), ('dbo:Person', 6.69498107), ('dbo:Group', 6.58890151), ('dbo:Organisation', 6.55622853), ('dbo:Activity', 6.2859847)]\n",
      "Actual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n",
      "\n",
      "ID: dbpedia_1335\n",
      "Entity Centric: [('dbo:LegalCase', 11.093895), ('dbo:Case', 11.093895), ('dbo:UnitOfWork', 11.093895), ('dbo:SocietalEvent', 10.37161), ('dbo:Event', 10.37161), ('dbo:PeriodicalLiterature', 10.369271000000001), ('dbo:Agent', 10.271667879166664), ('dbo:WrittenWork', 9.648729328571429), ('dbo:Person', 9.64732340681818), ('dbo:Wikidata:Q11424', 9.545929075)]\n",
      "Type Centric: [('dbo:Person', 6.19493142), ('dbo:Agent', 6.19039294), ('dbo:WrittenWork', 6.080408670000001), ('dbo:PeriodicalLiterature', 5.8841389500000005), ('dbo:SocietalEvent', 5.64750105), ('dbo:Wikidata:Q11424', 5.62163726), ('dbo:Event', 5.5810011799999995), ('dbo:Work', 5.35581235), ('dbo:LegalCase', 5.16034667), ('dbo:Case', 5.16034667)]\n",
      "Actual: ['dbo:Country', 'dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "\n",
      "ID: dbpedia_6016\n",
      "Entity Centric: [('dbo:Language', 12.650319), ('dbo:MeanOfTransportation', 12.139853), ('dbo:PeriodicalLiterature', 10.243072999999999), ('dbo:Stream', 10.129697333333333), ('dbo:WrittenWork', 10.08491325), ('dbo:BodyOfWater', 9.797270000000001), ('dbo:NaturalPlace', 9.797270000000001), ('dbo:Work', 9.459468168421052), ('dbo:Biomolecule', 9.401125), ('dbo:Animal', 9.356420328571428)]\n",
      "Type Centric: [('dbo:Species', 7.22235805), ('dbo:Eukaryote', 7.19849355), ('dbo:WrittenWork', 7.074630099999999), ('dbo:Animal', 7.06030419), ('dbo:Work', 6.95938795), ('dbo:Agent', 6.945807800000001), ('dbo:Person', 6.8744053), ('dbo:Organisation', 6.645820499999999), ('dbo:MeanOfTransportation', 6.6178422999999995), ('dbo:EducationalInstitution', 6.384739)]\n",
      "Actual: ['dbo:AcademicSubject', 'dbo:TopicalConcept']\n",
      "\n",
      "ID: dbpedia_3432\n",
      "Entity Centric: [('dbo:PeriodicalLiterature', 23.2606835), ('dbo:LegalCase', 17.0065375), ('dbo:Case', 17.0065375), ('dbo:UnitOfWork', 17.0065375), ('dbo:Cleric', 15.116182), ('dbo:WrittenWork', 14.746776615384618), ('dbo:EducationalInstitution', 14.2877625), ('dbo:Work', 14.103507555555552), ('dbo:Person', 13.755431281249997), ('dbo:Wikidata:Q11424', 13.70751)]\n",
      "Type Centric: [('dbo:Cleric', 15.789229689999997), ('dbo:Person', 14.23992987), ('dbo:Agent', 14.22497687), ('dbo:WrittenWork', 13.427858440000003), ('dbo:Work', 12.117561690000002), ('dbo:ArchitecturalStructure', 11.404403340000002), ('dbo:Location', 10.421906879999998), ('dbo:Place', 10.421701969999999), ('dbo:Organisation', 10.02177961), ('dbo:Building', 9.869098359999999)]\n",
      "Actual: ['dbo:Name']\n",
      "\n",
      "ID: dbpedia_16006\n",
      "Entity Centric: [('dbo:Wikidata:Q11424', 13.549900166666665), ('dbo:Company', 12.103516), ('dbo:Region', 12.076864), ('dbo:Cartoon', 11.765317), ('dbo:Athlete', 11.637418), ('dbo:Work', 11.61227572222222), ('dbo:Artist', 11.405987999999999), ('dbo:MeanOfTransportation', 11.39209), ('dbo:WrittenWork', 10.836153), ('dbo:PopulatedPlace', 10.710510499999998)]\n",
      "Type Centric: [('dbo:WrittenWork', 5.67229655), ('dbo:Wikidata:Q11424', 5.6659659300000005), ('dbo:Work', 5.6547414), ('dbo:Person', 5.60019721), ('dbo:Agent', 5.54251124), ('dbo:Group', 5.26174026), ('dbo:Politician', 5.22053293), ('dbo:Organisation', 5.165413430000001), ('dbo:FictionalCharacter', 5.1546985), ('dbo:Artist', 5.14531148)]\n",
      "Actual: ['dbo:Organisation', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_278\n",
      "Entity Centric: [('dbo:Organisation', 24.176065), ('dbo:Artist', 20.38513257142857), ('dbo:Person', 20.38513257142857), ('dbo:Wikidata:Q11424', 20.03655925490196), ('dbo:Work', 20.008273250000006), ('dbo:Agent', 19.892462333333327), ('dbo:MusicalWork', 18.565687)]\n",
      "Type Centric: [('dbo:MeanOfTransportation', 9.50595985), ('dbo:Artist', 8.77429241), ('dbo:Agent', 8.26057255), ('dbo:Wikidata:Q11424', 8.20969975), ('dbo:Activity', 8.1087917), ('dbo:Athlete', 7.87733867), ('dbo:Device', 7.21755001), ('dbo:Person', 7.126691800000001), ('dbo:Work', 6.95039701), ('dbo:Organisation', 6.895331189999999)]\n",
      "Actual: ['dbo:Person', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_7661\n",
      "Entity Centric: [('dbo:PeriodicalLiterature', 12.686056), ('dbo:Artist', 11.398296), ('dbo:Politician', 10.974059), ('dbo:Infrastructure', 10.73807), ('dbo:ArchitecturalStructure', 10.73807), ('dbo:Athlete', 10.3666369), ('dbo:Agent', 10.179057504166664), ('dbo:EducationalInstitution', 9.952432115384616), ('dbo:Person', 9.909534931914893), ('dbo:Organisation', 9.82032225)]\n",
      "Type Centric: [('dbo:Agent', 12.107197450000001), ('dbo:Politician', 11.40598218), ('dbo:Person', 11.4009322), ('dbo:WrittenWork', 11.088271599999999), ('dbo:Athlete', 10.838739799999999), ('dbo:Work', 10.62823712), ('dbo:Organisation', 9.20085803), ('dbo:EducationalInstitution', 7.45150535), ('dbo:PeriodicalLiterature', 7.2062078), ('dbo:Artist', 6.752239599999999)]\n",
      "Actual: ['dbo:Person', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_11196\n",
      "Entity Centric: [('dbo:Organisation', 13.598888300000002), ('dbo:Agent', 12.326561552325584), ('dbo:Person', 12.087653301724137), ('dbo:Place', 12.013907222222223), ('dbo:Location', 12.013907222222223), ('dbo:PopulatedPlace', 11.978019875), ('dbo:Genre', 11.911491), ('dbo:TopicalConcept', 11.911491), ('dbo:SocietalEvent', 11.894336), ('dbo:Event', 11.894336)]\n",
      "Type Centric: [('dbo:Person', 8.43957568), ('dbo:Cleric', 8.30541286), ('dbo:Agent', 8.30122778), ('dbo:SocietalEvent', 8.12824289), ('dbo:Event', 8.09266712), ('dbo:WrittenWork', 7.456076700000001), ('dbo:PopulatedPlace', 7.32891707), ('dbo:Location', 7.227393419999999), ('dbo:Place', 7.22737371), ('dbo:Politician', 6.886482300000001)]\n",
      "Actual: ['dbo:WrittenWork', 'dbo:Work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (key, val) in enumerate(query_scores.items()):\n",
    "    print(\"ID: {}\".format(key))\n",
    "    print(\"Entity Centric: {}\".format(val['entity_centric'][:10]))\n",
    "    print(\"Type Centric: {}\".format(val['type_centric'][:10]))\n",
    "    print(\"Actual: {}\\n\".format(val['actual']))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(predicted, ground_truth):\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def recall(predicted, ground_truth):\n",
    "    if len(ground_truth) == 0: return 0\n",
    "    correct = 0\n",
    "    for t in ground_truth:\n",
    "        if t in predicted:\n",
    "            correct += 1\n",
    "    return correct/len(ground_truth)\n",
    "\n",
    "\n",
    "def avg_precision(predicted, ground_truth):\n",
    "    precision = []\n",
    "    correct = 0\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            correct += 1\n",
    "            precision.append(correct/(i+1))\n",
    "\n",
    "    if len(precision) == 0: return 0\n",
    "    return sum(precision)/len(precision)\n",
    "\n",
    "def mean(predictions, ground_truths, func):\n",
    "    stats = []\n",
    "    for i in range(len(predictions)):\n",
    "        stats.append(func(predictions[i], ground_truths[i]))\n",
    "    return sum(stats)/len(stats) if len(stats) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Centric\n",
      "Recall: 0.6713697248220691\n",
      "MRR: 0.22355451968797388\n",
      "Mean Precision: 0.22355451968797388\n",
      "\n",
      "Type Centric\n",
      "Recall: 0.722420660867124\n",
      "MRR: 0.4080946288914935\n",
      "Mean Precision: 0.4080946288914935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ec_preds = []\n",
    "tc_preds = []\n",
    "ground_truths = []\n",
    "for _, val in query_scores.items():\n",
    "    ec_preds.append([x[0] for x in val['entity_centric']])\n",
    "    tc_preds.append([x[0] for x in val['type_centric']])\n",
    "    ground_truths.append(val['actual'])\n",
    "\n",
    "m_recall_ec = mean(ec_preds, ground_truths ,recall)\n",
    "mrr_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "m_recall_tc = mean(tc_preds, ground_truths ,recall)\n",
    "mrr_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "print(\"Entity Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_ec))\n",
    "print(\"MRR: {}\".format(mrr_ec))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_ec))\n",
    "\n",
    "print(\"Type Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_tc))\n",
    "print(\"MRR: {}\".format(mrr_tc))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_tc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type label features\n",
    "\n",
    "Functions used to populate type label features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_type_feature (t_terms,queries,t_features):\n",
    "    idf=[]\n",
    "    \n",
    "    if t_terms is not None:\n",
    "        num_doc = len(queries)\n",
    "        t_features['length'] = sum(len(i) for i in t_terms)\n",
    "        \n",
    "        for t in t_terms:\n",
    "            df = 0\n",
    "            for key in queries:\n",
    "                if t in queries[key]: \n",
    "                    df=df+1 \n",
    "            \n",
    "            if df == 0: idf.append(0)\n",
    "            else: idf.append(math.log(num_doc)/df)\n",
    "\n",
    "        if len(idf)>0:\n",
    "            t_features['sum_idf'] = sum(idf)\n",
    "            t_features['avg_idf'] = sum(idf)/len(t_terms)\n",
    "        else:\n",
    "            t_features['sum_idf'] = 0\n",
    "            t_features['avg_idf'] = 0\n",
    "        \n",
    "    else:\n",
    "        t_features['sum_idf'] = 0\n",
    "        t_features['avg_idf'] = 0\n",
    "        t_features['length'] = 0\n",
    "        \n",
    "    return t_features\n",
    "\n",
    "def jterms (t_terms,q_terms,t_features):\n",
    "    if t_terms is not None:\n",
    "        t_terms = set(t_terms)\n",
    "        q_terms = set([item for sublist in q_terms for item in sublist])\n",
    "        t_features['jsim'] = float(len(t_terms.intersection(q_terms))) / (len(t_terms) + len(q_terms) - len(t_terms.intersection(q_terms)))\n",
    "    else:\n",
    "        t_features['jsim']= 0\n",
    "\n",
    "    return t_features\n",
    "\n",
    "def similarity(t_terms, q_term, model, t_features):\n",
    "    similarity = []\n",
    "    #q = list(chain.from_iterable(term))\n",
    "    for q in q_term:\n",
    "        if q is not None:\n",
    "            for t in t_terms:\n",
    "                try:\n",
    "                    similarity.append(model.wv.similarity(t,q))\n",
    "                except:\n",
    "                    similarity.append(0)\n",
    "    \n",
    "    if len(similarity) >0:\n",
    "        t_features['avg_sim'] = sum(similarity)/len(similarity)\n",
    "    else:\n",
    "        t_features['avg_sim'] = 0\n",
    "        \n",
    "    return t_features\n",
    "\n",
    "def gettypelabelterm(dbotype):\n",
    "    t_term = [t.lower() for t in re.findall('.[^A-Z]*',dbotype)[1:]]\n",
    "    \n",
    "    return t_term\n",
    "\n",
    "def getqueryterm(queries):\n",
    "    q_terms = {}\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    for query in queries:\n",
    "        if query['question']:\n",
    "            question = query['question'].lower()\n",
    "            question = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]', '', question).split(' ')\n",
    "            question = [q for q in question if not q in stop_words]\n",
    "            question =[q for q in question if len(q)>1]           \n",
    "            q_terms[query['id']]=question\n",
    "    return q_terms\n",
    "\n",
    "def typelabel_features(t_term, q_term, queries, w2vm\n",
    "    '''input: dbotype query term, and query collection'''\n",
    "    typefeature = {'sum_idf','avg_idf','length','jsim','avg_sim'}\n",
    "    t_features={}\n",
    "    \n",
    "    #for q_term in q_terms:\n",
    "    t_features = idf_type_feature (t_term, queries, t_features)\n",
    "    t_features = jterms(t_term, q_term, t_features)\n",
    "    t_features = similarity(t_term, q_term, w2vmodel, t_features)\n",
    "    return list(t_features.values())\n",
    "\n",
    "def get_features(t_term,q_term, queries, w2vmodel,baselinefeatures):\n",
    "    '''combine both type label and baseline features '''\n",
    "    typefeatures = typelabel_features(t_term, q_term, queries, w2vmodel) \n",
    "    \n",
    "    return typefeatures + baselinefeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_trainingdata(TRAIN_QUERY):\n",
    "    feature = []\n",
    "    y = []\n",
    "    w2vmodel = api.load('word2vec-google-news-300')\n",
    "    queries = getqueryterm(TRAIN_QUERY)\n",
    "    \n",
    "    for query in Qdbpedia:\n",
    "        qtypes = query['type']\n",
    "        if query is not None:\n",
    "            \n",
    "        #call the index functions to get dbotypes and scores, and combine--Erik\n",
    "        #dbotypes, scores = ...\n",
    "        #for each dbotype from the index generate features and prediction\n",
    "            for dbotype in dbotypes:\n",
    "                t_term = gettypelabelterm(typ)\n",
    "                q_term = queries.get(query['id'],None)\n",
    "                feature.append(get_features(t_term, q_term, queries, w2vmodel, scores[dbotype]))\n",
    "                if dbotype in qtypes:\n",
    "                    y.append(1)\n",
    "                else:\n",
    "                    y.append(0)\n",
    "                    \n",
    "    return feature, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fwibawa\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train = prepare_trainingdata(TRAIN_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4431 4431\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train),len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
