{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages:\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import json\n",
    "import bz2\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query training dataset from Github\n",
    "\n",
    "Download query from github and split into train set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "14056\n"
     ]
    }
   ],
   "source": [
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/datasets/DBpedia/'\n",
    "file = 'smarttask_dbpedia_train.json'\n",
    "url = url + file\n",
    "queries=requests.get(url).json()\n",
    "\n",
    "\n",
    "random.seed(a=1234567)\n",
    "TRAIN_SIZE = int(len(queries) * 0.8)\n",
    "\n",
    "TRAIN_QUERY = queries[:TRAIN_SIZE]\n",
    "TEST_QUERY = queries[TRAIN_SIZE:]\n",
    "\n",
    "print(len(TRAIN_QUERY))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query stats\n",
    "#query = Counter([q['category'] for q in TRAIN_QUERY])\n",
    "#print(query)\n",
    "\n",
    "#resource_query = [q['type'] for q in TRAIN_QUERY if q['category']=='resource']\n",
    "#query_type = Counter(sum(resource_query,[]))\n",
    "\n",
    "#print(resource_query)\n",
    "#c0=0\n",
    "#c1=0\n",
    " \n",
    "#for q in resource_query:\n",
    "#    if len(resource_query[q])>1: c1 += 1\n",
    "#    else: c0 += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download file from DBPedia NLP dataset\n",
    "\n",
    "Check whether dataset files (bz2) are available in data folder and download bz2 file if they are not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dburl= 'http://downloads.dbpedia.org/2016-10/core-i18n/en/'\n",
    "dbfiles = ['long_abstracts_en.ttl.bz2','instance_types_transitive_en.ttl.bz2']\n",
    "type_file = 'dbpedia_types.tsv'\n",
    "dirname = 'data'\n",
    "\n",
    "for dbfile in dbfiles:\n",
    "    if not os.path.exists(dirname):\n",
    "        os.mkdir(dirname) \n",
    "    \n",
    "    # Download file\n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "        url = dburl + dbfile\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "   \n",
    "    if not os.path.exists(dirname + '/' + dbfile):\n",
    "\n",
    "        r = requests.get(url)\n",
    "        with open(dirname + '/'+ dbfile, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "\n",
    "    #extract\n",
    "    if not os.path.exists(dirname + '/' + dbfile[:-4]):\n",
    "        filepath= dirname + '/'+ dbfile\n",
    "        newfilepath = filepath[:-4] \n",
    "        with open(filepath, 'rb') as source, open(newfilepath, 'wb') as dest:\n",
    "            dest.write(bz2.decompress(source.read()))\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "url = 'https://raw.githubusercontent.com/smart-task/smart-dataset/master/evaluation/dbpedia/dbpedia_types.tsv'\n",
    "if not os.path.exists(dirname + '/' + type_file):\n",
    "    r = requests.get(url)\n",
    "    with open(dirname + '/'+ type_file, 'wb') as f:\n",
    "        f.write(r.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Index configuration \n",
    "\n",
    "For each of the fields, store the term vectors. These should be stored in the index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()\n",
    "INDEX_NAME_ENTITY = 'nlp_entity'\n",
    "\n",
    "INDEX_SETTINGS_ENTITY = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'types': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "INDEX_NAME_TYPE = 'nlp_type'\n",
    "\n",
    "INDEX_SETTINGS_TYPE = {\n",
    "    'mappings': {\n",
    "            'properties': {\n",
    "                'description': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "                'question': {\n",
    "                    'type': 'text',\n",
    "                    'term_vector': 'yes',\n",
    "                    'analyzer': 'english'\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABSTRACTS = '<http://dbpedia.org/ontology/abstract>'\n",
    "COMMENTS = '<http://www.w3.org/2000/01/rdf-schema#comment>'\n",
    "TYPES = '<http://www.w3.org/1999/02/22-rdf-syntax-ns#type>'\n",
    "abstract_files = ['long_abstracts_en.ttl']\n",
    "type_files = ['instance_types_transitive_en.ttl']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_abstract(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    text = text[1].replace('\"', '')\n",
    "    text = text.replace('@en .','')\n",
    "    return text\n",
    "\n",
    "def resolve_entity(text):\n",
    "    name = text.split('>')[0].replace('_', ' ')\n",
    "    name = name.split('/')[-1]\n",
    "    return name\n",
    "\n",
    "def resolve_types(text,delimiter):\n",
    "    text = text.split(delimiter)\n",
    "    types = text[1]    \n",
    "    types = types[types.rfind('/')+1:types.rfind('>')]\n",
    "    return ['dbo:'+types]\n",
    "\n",
    "def populate_type(filename,entity_doc):\n",
    "    #doc structure = {doc: {types: , abstract:}}\n",
    "    with open(filename,'r',encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            #skip header\n",
    "            \n",
    "            if i == 1000000: break\n",
    "            if i == 0: continue        \n",
    "            #only consider dbpedia\n",
    "            if 'dbpedia.org/ontology' in line.lower(): \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line)\n",
    "                if entity is not None:\n",
    "                    if TYPES in line:   \n",
    "                        types=resolve_types(line,TYPES) \n",
    "                        #add value or create new id in dictionary\n",
    "                        if entity in entity_doc:\n",
    "                            entity_doc[entity]['types']+= types\n",
    "                        else:\n",
    "                            entity_doc[entity]={'types':types}\n",
    "                        \n",
    "            \n",
    "            \n",
    "    return entity_doc\n",
    "\n",
    "def populate_abstract(filename, entity_doc):\n",
    "    #doc structure = {doc: {types: , abstract:}}\n",
    "    with open(filename, 'r', encoding ='utf-8') as bzfin:\n",
    "        for i, line in enumerate(bzfin):\n",
    "            \n",
    "            if i == 1000000: break\n",
    "            #skip header\n",
    "            if i == 0: continue      \n",
    "            #only consider dbpedia, in english\n",
    "            if 'dbpedia.org/ontology' in line.lower() and 'en' in line.lower():            \n",
    "                line = line.rstrip()\n",
    "                entity = resolve_entity(line) \n",
    "                if entity is not None:\n",
    "                    if ABSTRACTS in line:     \n",
    "                        desc = resolve_abstract(line,ABSTRACTS)\n",
    "                        if entity in entity_doc:\n",
    "                            if entity_doc[entity]['types'] is not None:\n",
    "                                entity_doc[entity]['description']= entity_doc[entity].get('description','') + desc\n",
    "                            else:                             \n",
    "                                continue\n",
    "                        else:\n",
    "                            continue\n",
    "            \n",
    "    return entity_doc\n",
    "\n",
    "def prepare_doc(type_files,abstract_files):\n",
    "    entity_doc={}\n",
    "    type_doc = {}\n",
    "    dirname ='data'\n",
    "    for file in type_files:\n",
    "        file = dirname + '/'+ file\n",
    "        entity_doc = populate_type(file,entity_doc)\n",
    "    print('type files are read')\n",
    "    for file in abstract_files:\n",
    "        file = dirname + '/'+ file\n",
    "        entity_doc = populate_abstract( file,entity_doc)\n",
    "    print('abstract files are read')\n",
    "    print('done populating entity doc')\n",
    "    #build type_doc from entity_doc\n",
    "    for entity in entity_doc:\n",
    "        types = entity_doc[entity]['types']\n",
    "        if types is not None:\n",
    "            for typ in types:\n",
    "                if typ in type_doc:\n",
    "                    type_doc[typ]['description']= type_doc[typ].get('description','') + entity_doc[entity].get('description','')\n",
    "                else:\n",
    "                    type_doc[typ] = {'description': entity_doc[entity].get('description','')}\n",
    "    \n",
    "    print('done populating type doc')\n",
    "    \n",
    "    #export to json\n",
    "    jsn = json.dumps(entity_doc)\n",
    "    f = open(\"entity_doc.json\",\"w\")\n",
    "    f.write(jsn)\n",
    "    f.close()\n",
    "    \n",
    "    jsn = json.dumps(type_doc)\n",
    "    f = open(\"type_doc.json\",\"w\")\n",
    "    f.write(jsn)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    return entity_doc, type_doc\n",
    "        \n",
    "\n",
    "def create_index_bulk(INDEX_NAME_ENTITY,INDEX_NAME_TYPE,type_files,abstract_files):   \n",
    "    entity_docs, type_docs = prepare_doc(type_files,abstract_files)\n",
    "    action_entity = [\n",
    "    {\n",
    "    '_id' : ids,\n",
    "    \"_source\" : entity_docs[ids]\n",
    "    }\n",
    "    for ids in entity_docs\n",
    "    ]\n",
    "    bulk = helpers.bulk(es,action_entity, index = INDEX_NAME_ENTITY, chunk_size=1000, request_timeout=200)\n",
    "        \n",
    "    print('entity doc are indexed')\n",
    "    action_type = [\n",
    "    {\n",
    "    '_id' : ids,\n",
    "    \"_source\" : type_docs[ids]\n",
    "    }\n",
    "    for ids in type_docs\n",
    "    ]\n",
    "    bulk = helpers.bulk(es,action_type, index = INDEX_NAME_TYPE, chunk_size=1000, request_timeout=200)\n",
    "    print('type doc are indexed')\n",
    "\n",
    "def populate_question(queries, INDEX_NAME_TYPE):\n",
    "    type_dict = defaultdict(str)\n",
    "    for query in queries:\n",
    "        if query['question'] == None:\n",
    "            continue\n",
    "        for t in query['type']:\n",
    "            type_dict[t] += query['question']\n",
    "    for t, q in type_dict.items():\n",
    "        if es.exists(index=INDEX_NAME_TYPE, id=t):\n",
    "            es.update(index=INDEX_NAME_TYPE, id=t, body={\"doc\":{\"question\": type_dict[t]}})\n",
    "        else:\n",
    "            es.index(index=INDEX_NAME_TYPE, id=t, body={\"doc\":{\"question\": type_dict[t]}})\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate index\n",
    "\n",
    "Declare two new indexes, one for entity centric model and the other for type centric model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_entity'}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME_ENTITY):\n",
    "    es.indices.delete(index=INDEX_NAME_ENTITY)\n",
    "es.indices.create(index=INDEX_NAME_ENTITY, body=INDEX_SETTINGS_ENTITY)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nlp_type'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "if es.indices.exists(INDEX_NAME_TYPE):\n",
    "    es.indices.delete(index=INDEX_NAME_TYPE)\n",
    "es.indices.create(index=INDEX_NAME_TYPE, body=INDEX_SETTINGS_TYPE)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "type files are read\n",
      "abstract files are read\n",
      "done populating entity doc\n",
      "done populating type doc\n",
      "entity doc are indexed\n",
      "type doc are indexed\n"
     ]
    }
   ],
   "source": [
    "create_index_bulk(INDEX_NAME_ENTITY,INDEX_NAME_TYPE,type_files,abstract_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "populate_question(TRAIN_QUERY, INDEX_NAME_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24848932\n",
      "['dbo:Region', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "[{'epoch': '1604750138', 'timestamp': '11:55:38', 'count': '120104'}]\n",
      "[{'epoch': '1604750138', 'timestamp': '11:55:38', 'count': '97'}]\n"
     ]
    }
   ],
   "source": [
    "#check indexes\n",
    "res = es.get(index='nlp_type', id='dbo:Place')['_source']['description']\n",
    "print(len(res))\n",
    "res = es.get(index='nlp_entity', id='Alabama')['_source']['types']\n",
    "print(res)\n",
    "count = es.cat.count('nlp_entity', params={\"format\": \"json\"})\n",
    "print(count)\n",
    "count = es.cat.count('nlp_type', params={\"format\": \"json\"})\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predic types for queries\n",
    "\n",
    "\n",
    "First score documents given the querie. Count the number of times a type occurs across the documents, dividing it by the sum of all type counts to get the type weight. For every type in a document, multiply the type weight with the document score and add to the types score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_query(es, query):\n",
    "    tokens = es.indices.analyze(index=INDEX_NAME_TYPE, body={'text': query})['tokens']\n",
    "    query_terms = []\n",
    "    for t in sorted(tokens, key=lambda x: x['position']):\n",
    "        query_terms.append(t['token'])\n",
    "    return query_terms\n",
    "\n",
    "def entity_centric_scorer(index_name, query, field='description', k=100):\n",
    "    es_query = {\n",
    "        \"query\":{\n",
    "            \"query_string\":{\n",
    "                \"query\": query,\n",
    "                \"default_field\": field\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    matches = es.search(index=index_name, body=es_query, _source=True, size=k)['hits']['hits']\n",
    "\n",
    "    type_count = defaultdict(int)\n",
    "    for match in matches:\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_count[doc_type] += 1\n",
    "\n",
    "    type_weight = {}\n",
    "    for t, c in type_count.items():\n",
    "        type_weight[t] = 1/c\n",
    "\n",
    "    type_score = defaultdict(int)\n",
    "    for match in matches:\n",
    "        doc_id = match['_id']\n",
    "        doc_score = match['_score']\n",
    "        for doc_type in match['_source']['types']:\n",
    "            type_score[doc_type] += doc_score * type_weight[doc_type]\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_score.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def type_centric_scorer(index_name, q_terms, field='description', k=100):\n",
    "    type_scores = defaultdict(int)\n",
    "    for term in q_terms:\n",
    "        es_query = {\n",
    "            \"query\":{\n",
    "                \"query_string\":{\n",
    "                    \"query\": term,\n",
    "                    \"default_field\": field\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        matches = es.search(index=index_name, body=es_query, _source=False, size=k)['hits']['hits']\n",
    "        for match in matches:\n",
    "            type_scores[match['_id']] += match['_score']\n",
    "\n",
    "    scores = sorted([(t, s) for t, s in type_scores.items()],key=lambda tup: tup[1], reverse=True)\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_scores = {}\n",
    "pattern = re.compile(r'[\\W_]+')\n",
    "for query in TRAIN_QUERY:\n",
    "    if query['category'] != 'resource':\n",
    "        continue\n",
    "    if query['question'] == None:\n",
    "        continue\n",
    "    q = pattern.sub(' ', query['question'])\n",
    "    ec_pred = entity_centric_scorer(INDEX_NAME_ENTITY, q)\n",
    "    q_terms = analyze_query(es, q)\n",
    "    tc_pred = type_centric_scorer(INDEX_NAME_TYPE, q_terms)\n",
    "    query_scores[query['id']] = {'entity_centric': ec_pred, 'type_centric': tc_pred, 'actual': query['type']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: dbpedia_14427\n",
      "Entity Centric: [('dbo:Group', 15.700501499999998), ('dbo:Cartoon', 14.047559), ('dbo:Organisation', 13.0069865), ('dbo:RouteOfTransportation', 12.868805), ('dbo:Infrastructure', 12.868805), ('dbo:MusicalWork', 10.383891444444444), ('dbo:Agent', 10.352069909090911), ('dbo:Work', 10.320312022222225), ('dbo:WrittenWork', 10.244019), ('dbo:Software', 10.227817)]\n",
      "Type Centric: [('dbo:MusicalWork', 6.932363639999999), ('dbo:Work', 6.85984862), ('dbo:WrittenWork', 6.844822239999999), ('dbo:Group', 6.74023693), ('dbo:Agent', 6.6516779), ('dbo:ArchitecturalStructure', 6.5886717), ('dbo:Wikidata:Q11424', 6.58418887), ('dbo:Person', 6.559901190000001), ('dbo:Artist', 6.54073076), ('dbo:Location', 6.474589539999999)]\n",
      "Actual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n",
      "\n",
      "ID: dbpedia_3681\n",
      "Entity Centric: [('dbo:Food', 13.540861), ('dbo:Artist', 12.90148), ('dbo:Company', 11.979504888888886), ('dbo:Agent', 11.927975352631583), ('dbo:Person', 11.912545999999999), ('dbo:Organisation', 11.718885999999998), ('dbo:Broadcaster', 11.518802), ('dbo:PopulatedPlace', 10.90964825), ('dbo:Place', 10.90964825), ('dbo:Location', 10.90964825)]\n",
      "Type Centric: [('dbo:Agent', 9.28586611), ('dbo:Company', 6.81826095), ('dbo:Work', 6.766858919999999), ('dbo:Organisation', 6.74814782), ('dbo:Person', 6.666759020000001), ('dbo:MusicalWork', 6.58095596), ('dbo:WrittenWork', 6.5111526), ('dbo:Group', 6.41623298), ('dbo:Wikidata:Q11424', 6.379384379999999), ('dbo:PeriodicalLiterature', 6.2533329)]\n",
      "Actual: ['dbo:EducationalInstitution', 'dbo:Organisation', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_12020\n",
      "Entity Centric: [('dbo:ArchitecturalStructure', 12.466058), ('dbo:Organisation', 12.3633530625), ('dbo:Place', 12.340116870967748), ('dbo:Location', 12.340116870967748), ('dbo:PopulatedPlace', 12.335918833333333), ('dbo:Settlement', 12.254845277777775), ('dbo:Agent', 12.055326434210524), ('dbo:Politician', 11.999879857142856), ('dbo:Person', 11.937496068965515), ('dbo:Region', 11.85902769047619)]\n",
      "Type Centric: [('dbo:Region', 7.92786691), ('dbo:Politician', 7.9090586), ('dbo:PopulatedPlace', 7.761287729999999), ('dbo:Location', 7.6931470299999996), ('dbo:Place', 7.693122939999999), ('dbo:Settlement', 7.66724338), ('dbo:Person', 7.58539858), ('dbo:Agent', 7.50045232), ('dbo:Organisation', 7.22019414), ('dbo:ArchitecturalStructure', 6.73481116)]\n",
      "Actual: ['dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "\n",
      "ID: dbpedia_10315\n",
      "Entity Centric: [('dbo:WrittenWork', 16.777676), ('dbo:Cartoon', 15.164494), ('dbo:Work', 15.124240666666665), ('dbo:Language', 14.219619), ('dbo:Animal', 13.531187), ('dbo:Eukaryote', 13.531187), ('dbo:Species', 13.531187), ('dbo:TimePeriod', 13.467945414893615), ('dbo:PopulatedPlace', 13.373524), ('dbo:Place', 13.373524)]\n",
      "Type Centric: [('dbo:Work', 7.142646150000001), ('dbo:MusicalWork', 7.029408179999999), ('dbo:WrittenWork', 7.015293870000001), ('dbo:Wikidata:Q11424', 6.96027084), ('dbo:Artist', 6.95725148), ('dbo:Agent', 6.83414707), ('dbo:Person', 6.69498107), ('dbo:Group', 6.58890151), ('dbo:Organisation', 6.55622853), ('dbo:Activity', 6.2859847)]\n",
      "Actual: ['dbo:Opera', 'dbo:MusicalWork', 'dbo:Work']\n",
      "\n",
      "ID: dbpedia_1335\n",
      "Entity Centric: [('dbo:LegalCase', 11.093895), ('dbo:Case', 11.093895), ('dbo:UnitOfWork', 11.093895), ('dbo:SocietalEvent', 10.37161), ('dbo:Event', 10.37161), ('dbo:PeriodicalLiterature', 10.369271000000001), ('dbo:Agent', 10.271667879166664), ('dbo:WrittenWork', 9.648729328571429), ('dbo:Person', 9.64732340681818), ('dbo:Wikidata:Q11424', 9.545929075)]\n",
      "Type Centric: [('dbo:Person', 6.19493142), ('dbo:Agent', 6.19039294), ('dbo:WrittenWork', 6.080408670000001), ('dbo:PeriodicalLiterature', 5.8841389500000005), ('dbo:SocietalEvent', 5.64750105), ('dbo:Wikidata:Q11424', 5.62163726), ('dbo:Event', 5.5810011799999995), ('dbo:Work', 5.35581235), ('dbo:LegalCase', 5.16034667), ('dbo:Case', 5.16034667)]\n",
      "Actual: ['dbo:Country', 'dbo:State', 'dbo:PopulatedPlace', 'dbo:Place', 'dbo:Location']\n",
      "\n",
      "ID: dbpedia_6016\n",
      "Entity Centric: [('dbo:Language', 12.650319), ('dbo:MeanOfTransportation', 12.139853), ('dbo:PeriodicalLiterature', 10.243072999999999), ('dbo:Stream', 10.129697333333333), ('dbo:WrittenWork', 10.08491325), ('dbo:BodyOfWater', 9.797270000000001), ('dbo:NaturalPlace', 9.797270000000001), ('dbo:Work', 9.459468168421052), ('dbo:Biomolecule', 9.401125), ('dbo:Animal', 9.356420328571428)]\n",
      "Type Centric: [('dbo:Species', 7.22235805), ('dbo:Eukaryote', 7.19849355), ('dbo:WrittenWork', 7.074630099999999), ('dbo:Animal', 7.06030419), ('dbo:Work', 6.95938795), ('dbo:Agent', 6.945807800000001), ('dbo:Person', 6.8744053), ('dbo:Organisation', 6.645820499999999), ('dbo:MeanOfTransportation', 6.6178422999999995), ('dbo:EducationalInstitution', 6.384739)]\n",
      "Actual: ['dbo:AcademicSubject', 'dbo:TopicalConcept']\n",
      "\n",
      "ID: dbpedia_3432\n",
      "Entity Centric: [('dbo:PeriodicalLiterature', 23.2606835), ('dbo:LegalCase', 17.0065375), ('dbo:Case', 17.0065375), ('dbo:UnitOfWork', 17.0065375), ('dbo:Cleric', 15.116182), ('dbo:WrittenWork', 14.746776615384618), ('dbo:EducationalInstitution', 14.2877625), ('dbo:Work', 14.103507555555552), ('dbo:Person', 13.755431281249997), ('dbo:Wikidata:Q11424', 13.70751)]\n",
      "Type Centric: [('dbo:Cleric', 15.789229689999997), ('dbo:Person', 14.23992987), ('dbo:Agent', 14.22497687), ('dbo:WrittenWork', 13.427858440000003), ('dbo:Work', 12.117561690000002), ('dbo:ArchitecturalStructure', 11.404403340000002), ('dbo:Location', 10.421906879999998), ('dbo:Place', 10.421701969999999), ('dbo:Organisation', 10.02177961), ('dbo:Building', 9.869098359999999)]\n",
      "Actual: ['dbo:Name']\n",
      "\n",
      "ID: dbpedia_16006\n",
      "Entity Centric: [('dbo:Wikidata:Q11424', 13.549900166666665), ('dbo:Company', 12.103516), ('dbo:Region', 12.076864), ('dbo:Cartoon', 11.765317), ('dbo:Athlete', 11.637418), ('dbo:Work', 11.61227572222222), ('dbo:Artist', 11.405987999999999), ('dbo:MeanOfTransportation', 11.39209), ('dbo:WrittenWork', 10.836153), ('dbo:PopulatedPlace', 10.710510499999998)]\n",
      "Type Centric: [('dbo:WrittenWork', 5.67229655), ('dbo:Wikidata:Q11424', 5.6659659300000005), ('dbo:Work', 5.6547414), ('dbo:Person', 5.60019721), ('dbo:Agent', 5.54251124), ('dbo:Group', 5.26174026), ('dbo:Politician', 5.22053293), ('dbo:Organisation', 5.165413430000001), ('dbo:FictionalCharacter', 5.1546985), ('dbo:Artist', 5.14531148)]\n",
      "Actual: ['dbo:Organisation', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_278\n",
      "Entity Centric: [('dbo:Organisation', 24.176065), ('dbo:Artist', 20.38513257142857), ('dbo:Person', 20.38513257142857), ('dbo:Wikidata:Q11424', 20.03655925490196), ('dbo:Work', 20.008273250000006), ('dbo:Agent', 19.892462333333327), ('dbo:MusicalWork', 18.565687)]\n",
      "Type Centric: [('dbo:MeanOfTransportation', 9.50595985), ('dbo:Artist', 8.77429241), ('dbo:Agent', 8.26057255), ('dbo:Wikidata:Q11424', 8.20969975), ('dbo:Activity', 8.1087917), ('dbo:Athlete', 7.87733867), ('dbo:Device', 7.21755001), ('dbo:Person', 7.126691800000001), ('dbo:Work', 6.95039701), ('dbo:Organisation', 6.895331189999999)]\n",
      "Actual: ['dbo:Person', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_7661\n",
      "Entity Centric: [('dbo:PeriodicalLiterature', 12.686056), ('dbo:Artist', 11.398296), ('dbo:Politician', 10.974059), ('dbo:Infrastructure', 10.73807), ('dbo:ArchitecturalStructure', 10.73807), ('dbo:Athlete', 10.3666369), ('dbo:Agent', 10.179057504166664), ('dbo:EducationalInstitution', 9.952432115384616), ('dbo:Person', 9.909534931914893), ('dbo:Organisation', 9.82032225)]\n",
      "Type Centric: [('dbo:Agent', 12.107197450000001), ('dbo:Politician', 11.40598218), ('dbo:Person', 11.4009322), ('dbo:WrittenWork', 11.088271599999999), ('dbo:Athlete', 10.838739799999999), ('dbo:Work', 10.62823712), ('dbo:Organisation', 9.20085803), ('dbo:EducationalInstitution', 7.45150535), ('dbo:PeriodicalLiterature', 7.2062078), ('dbo:Artist', 6.752239599999999)]\n",
      "Actual: ['dbo:Person', 'dbo:Agent']\n",
      "\n",
      "ID: dbpedia_11196\n",
      "Entity Centric: [('dbo:Organisation', 13.598888300000002), ('dbo:Agent', 12.326561552325584), ('dbo:Person', 12.087653301724137), ('dbo:Place', 12.013907222222223), ('dbo:Location', 12.013907222222223), ('dbo:PopulatedPlace', 11.978019875), ('dbo:Genre', 11.911491), ('dbo:TopicalConcept', 11.911491), ('dbo:SocietalEvent', 11.894336), ('dbo:Event', 11.894336)]\n",
      "Type Centric: [('dbo:Person', 8.43957568), ('dbo:Cleric', 8.30541286), ('dbo:Agent', 8.30122778), ('dbo:SocietalEvent', 8.12824289), ('dbo:Event', 8.09266712), ('dbo:WrittenWork', 7.456076700000001), ('dbo:PopulatedPlace', 7.32891707), ('dbo:Location', 7.227393419999999), ('dbo:Place', 7.22737371), ('dbo:Politician', 6.886482300000001)]\n",
      "Actual: ['dbo:WrittenWork', 'dbo:Work']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, (key, val) in enumerate(query_scores.items()):\n",
    "    print(\"ID: {}\".format(key))\n",
    "    print(\"Entity Centric: {}\".format(val['entity_centric'][:10]))\n",
    "    print(\"Type Centric: {}\".format(val['type_centric'][:10]))\n",
    "    print(\"Actual: {}\\n\".format(val['actual']))\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reciprocal_rank(predicted, ground_truth):\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            return 1/(i+1)\n",
    "    return 0\n",
    "\n",
    "def recall(predicted, ground_truth):\n",
    "    if len(ground_truth) == 0: return 0\n",
    "    correct = 0\n",
    "    for t in ground_truth:\n",
    "        if t in predicted:\n",
    "            correct += 1\n",
    "    return correct/len(ground_truth)\n",
    "\n",
    "\n",
    "def avg_precision(predicted, ground_truth):\n",
    "    precision = []\n",
    "    correct = 0\n",
    "    for i, t in enumerate(predicted):\n",
    "        if t in ground_truth:\n",
    "            correct += 1\n",
    "            precision.append(correct/(i+1))\n",
    "\n",
    "    if len(precision) == 0: return 0\n",
    "    return sum(precision)/len(precision)\n",
    "\n",
    "def mean(predictions, ground_truths, func):\n",
    "    stats = []\n",
    "    for i in range(len(predictions)):\n",
    "        stats.append(func(predictions[i], ground_truths[i]))\n",
    "    return sum(stats)/len(stats) if len(stats) > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Centric\n",
      "Recall: 0.6713697248220691\n",
      "MRR: 0.22355451968797388\n",
      "Mean Precision: 0.22355451968797388\n",
      "\n",
      "Type Centric\n",
      "Recall: 0.722420660867124\n",
      "MRR: 0.4080946288914935\n",
      "Mean Precision: 0.4080946288914935\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ec_preds = []\n",
    "tc_preds = []\n",
    "ground_truths = []\n",
    "for _, val in query_scores.items():\n",
    "    ec_preds.append([x[0] for x in val['entity_centric']])\n",
    "    tc_preds.append([x[0] for x in val['type_centric']])\n",
    "    ground_truths.append(val['actual'])\n",
    "\n",
    "m_recall_ec = mean(ec_preds, ground_truths ,recall)\n",
    "mrr_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_ec = mean(ec_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "m_recall_tc = mean(tc_preds, ground_truths ,recall)\n",
    "mrr_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "m_prec_tc = mean(tc_preds, ground_truths ,reciprocal_rank)\n",
    "\n",
    "print(\"Entity Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_ec))\n",
    "print(\"MRR: {}\".format(mrr_ec))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_ec))\n",
    "\n",
    "print(\"Type Centric\")\n",
    "print(\"Recall: {}\".format(m_recall_tc))\n",
    "print(\"MRR: {}\".format(mrr_tc))\n",
    "print(\"Mean Precision: {}\\n\".format(m_prec_tc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type label features\n",
    "\n",
    "Functions used to populate type label features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def typelabel_terms (filename):\n",
    "    '''function to obtain label of type t'''\n",
    "    t_label ={}\n",
    "    with open(filename, 'r') as tsv_file:\n",
    "        next(tsv_file)  # Skip header row\n",
    "        for line in tsv_file:\n",
    "            fields = line.rstrip().split('\\t')\n",
    "            type_name= fields[0]\n",
    "            if t_label.get(type_name,'') == '':\n",
    "                t_label[type_name] = re.findall('.[^A-Z]*',type_name)[1:]\n",
    "    return t_label\n",
    "\n",
    "def idf_type_feature (t_terms,queries,t_features):\n",
    "    idf=[]\n",
    "    q_terms =[]\n",
    "    \n",
    "    if t_terms is not None:\n",
    "        t_features['length'] = len(t_terms)\n",
    "        num_doc = len(queries)\n",
    "        for query in queries:\n",
    "            if query['question'] is not None:\n",
    "                q_terms.append(query['question'].lower())\n",
    "\n",
    "        for t in t_terms:\n",
    "            df = 0\n",
    "            for qt in q_terms:\n",
    "                if t.lower() in qt: df += 1          \n",
    "            idf.append(math.log(num_doc)/df)\n",
    "\n",
    "        if len(idf)>0:\n",
    "            t_features['query_sum_idf'] = sum(idf)\n",
    "            t_features['query_avg_idf'] = sum(idf)/len(t_terms)\n",
    "        else:\n",
    "            t_features['query_sum_idf'] = 0\n",
    "            t_features['query_avg_idf'] = 0\n",
    "        \n",
    "    else:\n",
    "        t_features['query_sum_idf'] = 0\n",
    "        t_features['query_avg_idf'] = 0\n",
    "        t_features['length'] = 0\n",
    "        \n",
    "    return t_features\n",
    "\n",
    "def jterms (t_terms,queries,t_features):\n",
    "    jsim =[]\n",
    "    q =[]\n",
    "    \n",
    "    if t_terms is not None:\n",
    "        for query in queries:\n",
    "            if query['question'] is not None:\n",
    "                question = query['question'].lower()\n",
    "                question = re.sub(r'[-()\\\"#/@;:<>{}`+=~|.!?,]', '', question).split(' ')\n",
    "                q.append(question)\n",
    "\n",
    "        t_terms =[t.lower() for t in t_terms]\n",
    "        t_terms = set(t_terms)\n",
    "\n",
    "        q_terms = set([item for sublist in q for item in sublist])\n",
    "        t_features['jsim'] = float(len(t_terms.intersection(q_terms))) / (len(t_terms) + len(q_terms) - len(t_terms.intersection(q_terms)))\n",
    "    else:\n",
    "        t_features['jsim']= 0\n",
    "\n",
    "    return t_features\n",
    "\n",
    "\n",
    "\n",
    "def typelabel_features(dbotype ,TRAIN_QUERY):\n",
    "    '''input: dbo type t whose features to be calculated'''\n",
    "    \n",
    "    t_features={}\n",
    "    type_file = 'dbpedia_types.tsv'\n",
    "    filename = 'data/' + type_file\n",
    "    t_terms = typelabel_terms (filename)\n",
    "    t_features = idf_type_feature (t_terms.get(dbotype, None), TRAIN_QUERY, t_features\n",
    "    t_features = jterms(t_terms.get(dbotype, None), TRAIN_QUERY, t_features)\n",
    "\n",
    "    return t_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'length': 2,\n",
       " 'query_sum_idf': 0.46306931538748763,\n",
       " 'query_avg_idf': 0.23153465769374382,\n",
       " 'jsim': 0.00010274852298998202}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "typelabel_features('dbo:BasketballLeague',TRAIN_QUERY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}